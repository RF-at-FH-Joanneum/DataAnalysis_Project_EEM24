{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis Assignment 4\n",
    "**Group: Ohm_Squad**\n",
    "\n",
    "**Members: Rauch,Bilijesko,Frizberg**\n",
    "\n",
    "**Datasets: Westermo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Configure plotting\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': [12, 8],\n",
    "    'figure.dpi': 150,\n",
    "    'figure.autolayout': True,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'font.size': 12\n",
    "})\n",
    "\n",
    "pathRaw = \"./data_raw/\"\n",
    "pathFilter = \"./data_filtered/\"\n",
    "pathProcessd = \"./data_processed/\"\n",
    "pathVisuRaw = \"./visu_raw/\"\n",
    "pathVisuProcessed = \"./visu_processed/\"\n",
    "pathOnlyProcessed = \"./visu_only_processed/\"\n",
    "pathProb = \"./visu_prob/\"\n",
    "\n",
    "files = [f\"system-{number}.csv\" for number in range(1, 20)]\n",
    "\n",
    "# Systems 3, 5, 6, 8, 11 and 17 do not have sys-thermal readings ! 3/5/6 -> crashes 8/11/17 -> no thermal\n",
    "remove_entries = [7,10,16]\n",
    "files = [item for index, item in enumerate(files) if index not in remove_entries]\n",
    "# --------------------- chosen for analysis ---------------------------------------------------------------------- \n",
    "# 19\n",
    "present = files[-1]\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Data Preprocessing and Basic Analysis\n",
    "- **Basic statistical analysis using pandas**\n",
    "> -> see load_system_data()\n",
    "- **Original data quality analysis (including visualization)**\n",
    "> -> see Analysis Notes after visu_raw_data()\n",
    "- **Data preprocessing**\n",
    "> -> see preprocess_system_data() and \"data_processed\"\n",
    "- **Preprocessed vs original data visual analysis**\n",
    "> -> see Analysis Notes after visu_processed_data()\n",
    "\n",
    "# 2.2 Visualization and Exploratory Analysis\n",
    "- **Time series visualizations**\n",
    "- **Distribution analysis with histograms**\n",
    "- **Correlation analysis and heatmaps**\n",
    "- **Daily pattern analysis**\n",
    "> -> see visu_processed_data() and Analysis Notes after visu_processed_data()\n",
    "- **Summary of observed patterns - similar to True/False questions**\n",
    "> -> see Analysis Notes after visu_processed_data()\n",
    "\n",
    ">All figures/plots can be accessed in \"visu_raw\", \"visu_processed\" and \"visu_only_processed\".\n",
    "\n",
    "\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Filtering\n",
    "Files are fetched from directory and prefiltering for columns of interst.\n",
    "\n",
    "Processing timestamps to datetime for usage in timeseries (and usability).\n",
    "\n",
    "Done via a function to execute for every file separately and be able to pipe if necessary.\n",
    "\n",
    "Returning the dataframe could be either dropped or caught by either a container or piped into the next function.\n",
    "\n",
    "- **2.3: Basic statistical analysis using pandas**\n",
    ">  -> output into CSV (visu_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_system_data(file_dir: str, file_name: str) -> pd.DataFrame :\n",
    "    \"\"\"Load and prepare test system performance data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_dir : str\n",
    "        Path to the CSV data file location (directory)\n",
    "    file_name : str\n",
    "        Name of the specified CSV file\n",
    "    \n",
    "    Additional outputs\n",
    "    saves filtered data into dir \"./data_filtered\"\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Raw dataframe with columns:\n",
    "        - datetime (index)\n",
    "        - load-15m\n",
    "        - memory_used_pct\n",
    "        - cpu-user\n",
    "        - cpu-system\n",
    "        - sys-thermal\n",
    "        - sys-interrupt-rate\n",
    "        - server-up\n",
    "        - disk-io-time\n",
    "    \"\"\"\n",
    "    file_path = file_dir + file_name\n",
    "\n",
    "    df = pd.read_csv(file_path, delimiter = \",\",usecols=[\"timestamp\",\n",
    "                                                         \"load-15m\",\n",
    "                                                         \"sys-mem-available\",\n",
    "                                                         \"sys-mem-total\",\n",
    "                                                         \"cpu-user\",\n",
    "                                                         \"cpu-system\",\n",
    "                                                         \"sys-thermal\",\n",
    "                                                         \"sys-interrupt-rate\",\n",
    "                                                         \"server-up\",\n",
    "                                                         \"disk-io-time\"]) # Read in data with columns\n",
    "    \n",
    "\n",
    "    \n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'], unit = 's', errors = 'coerce') # Create datetime from timestamp\n",
    "    \n",
    "    df.set_index('datetime', inplace=True) # Set datetime as index\n",
    "\n",
    "    df['memory_used_pct'] = (1 - df['sys-mem-available']/df['sys-mem-total']) * 100 # Memory usage calculation\n",
    "    df.drop([\"timestamp\",\"sys-mem-available\",\"sys-mem-total\"], axis=1, inplace=True) # Drop unneccessary data\n",
    "    \n",
    "    df.to_csv(pathFilter+file_name, index=True) # output filtered .csv to path (dir)\n",
    "    \n",
    "    file_name = file_name.replace(\".csv\",\"\")\n",
    "    df.describe().to_csv(f'{pathVisuRaw}{file_name}_desciption.csv') # output pandas desscribe() result into .csv to path (dir)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing \n",
    "df = load_system_data(pathRaw,present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre filter all files\n",
    "# for file in files:\n",
    "#     load_system_data(pathRaw, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Raw\n",
    "- **2.1: Original data quality analysis (including visualization)**\n",
    "- **2.2: Time series visualizations**\n",
    "- **2.2: Distribution analysis with histograms**\n",
    "- **2.2: Correlation analysis and heatmaps**\n",
    "- **2.2: Daily pattern analysis**\n",
    "\n",
    "First: Helper functions for interacting with images and os to delete temporary files.\n",
    "\n",
    "Second: Main function for visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted https://stackoverflow.com/questions/6996603/how-can-i-delete-a-file-or-folder-in-python\n",
    "def delete_images(files: list) -> None:\n",
    "    \"\"\"Deletes the files specified in the list of file paths.\n",
    "    Parameters\n",
    "    ----------\n",
    "    files: list[str]\n",
    "        List of names of image files to put into .pdf file. \n",
    "    \n",
    "    Additional output\n",
    "    ----------\n",
    "        Deltes list of images.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        None\n",
    "    \"\"\"\n",
    "    for file in files:\n",
    "        try:\n",
    "            if os.path.exists(file):\n",
    "                os.remove(file)\n",
    "                #print(f\"Deleted: {file}\")\n",
    "            else:\n",
    "                print(f\"File not found: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file}: {e}\")\n",
    "            \n",
    "# adapted https://stackoverflow.com/questions/40906463/png-images-to-one-pdf-in-python \n",
    "# and https://www.geeksforgeeks.org/save-multiple-matplotlib-figures-in-single-pdf-file-using-python/ \n",
    "def save_image(image_names: list, out_dir: str, filename: str) -> None: \n",
    "    \"\"\"Gathers multiple plt.figure obejcts and outputs thm into a .pdf \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image_names: list[str]\n",
    "        List of names of image files to put into .pdf file   \n",
    "    out_dir: str\n",
    "        Path to the directory of output .pdf file\n",
    "    filename: str\n",
    "        Name of output .pdf file\n",
    "        \n",
    "    Additional output\n",
    "    ----------\n",
    "    Saves a .pdf created by multiple .pngs into specified directory\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    image_list = [] #contains opened files\n",
    "    for name in image_names:\n",
    "        print(name)\n",
    "        image_list.append(Image.open(name))\n",
    "\n",
    "    image_list[0].save(f\"{out_dir}{filename}_allPlots.pdf\", save_all=True, append_images=image_list[1:])\n",
    "    for image in image_list:\n",
    "        image.close()\n",
    "    print(f\"{out_dir}{filename}_allPlots.pdf\")\n",
    "    delete_images(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visu_raw_data(show_plots: bool, file_dir: str, file_name: str, df_arg: pd.DataFrame, isRaw: bool = True):\n",
    "    \"\"\"Load and visualize filtered and processed test system performance data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    show_plots: bool\n",
    "        Just output files or display in notebook\n",
    "    file_dir : str\n",
    "        Path to the CSV data file location (directory)\n",
    "    file_name : str\n",
    "        Name of the specified CSV file\n",
    "    isRaw : bool (Default: True)\n",
    "        function can be used to visualize any raw or processed -> changes data_type (string) and out_dir (string)\n",
    "        \n",
    "    optional\n",
    "    df_arg: pd.DataFrame\n",
    "        output from load_system_data()\n",
    "\n",
    "    Additional output\n",
    "    ----------\n",
    "    saves visualized data into dir \"./visu_raw\" by calling save_image() and cleaning temp-files with delete_images()\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check DataFrame was passed\n",
    "    if isinstance(df_arg, pd.DataFrame):\n",
    "        df = df_arg\n",
    "        # File name and path -> pd used => no identifier => using \"./\" \n",
    "        out_dir = \"./\"\n",
    "        out_name = \"Visu_output_noident\"\n",
    "        print(\"Function called with a DataFrame.\")\n",
    "    else:\n",
    "        # Attempt to read the DataFrame from file\n",
    "        try:\n",
    "            file_path = file_dir + file_name\n",
    "            df = pd.read_csv(file_path, delimiter = \",\",usecols=[\"datetime\",\"load-15m\",\"memory_used_pct\",\"cpu-user\",\"cpu-system\",\"sys-thermal\",\"sys-interrupt-rate\",\"server-up\",\"disk-io-time\"])\n",
    "            print(f\"Function called with a file: {file_path}\")\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "            df.set_index('datetime', inplace=True)\n",
    "            # File name and path -> path used => use identifier \n",
    "            out_dir = pathVisuRaw\n",
    "            out_name = file_name.replace('.csv', '')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading the file: {e}\")\n",
    "            return None\n",
    "        \n",
    "    measurements = {\n",
    "        \"load-15m\": ('load-15m', '%'),\n",
    "        \"memory_used_pct\": ('memory_used_pct', '%'),\n",
    "        \"cpu-user\": ('cpu-user', 'delta-s'),\n",
    "        \"cpu-system\": ('cpu-system', 'delta-s'),\n",
    "        \"sys-thermal\": ('sys-thermal', 'avg delta-°C/min'),\n",
    "        \"sys-interrupt-rate\": ('sys-interrupt-rate', 'delta-s'),\n",
    "        \"disk-io-time\": ('disk-io-time', 'delta-s')\n",
    "        #,\"server-up\": ('server-sup', '')\n",
    "    }\n",
    "    if (isRaw):\n",
    "        data_type = \"Raw\"\n",
    "    else:\n",
    "        data_type = \"Processed\"\n",
    "        out_dir = pathOnlyProcessed\n",
    "    \n",
    "    image_names = []\n",
    "    image_nr = 0\n",
    "    \n",
    "    # Plot 1: Time-Series\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(15, 30))\n",
    "    fig.suptitle(f\"Tme-Series - {data_type} Data\", fontsize=16, y=1.02)\n",
    "   \n",
    "    for i,(measure, (title, unit)) in enumerate(measurements.items()): # iterate over all measurements\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "\n",
    "        df.iloc[::10].pivot(columns='server-up', values=measure).plot(ax=axes[row, col],alpha=0.7, linewidth=2,color=['red','blue']) # only use every 10th entry\n",
    "\n",
    "        axes[row, col].set_title(f'Time-Series of {measure.upper()}')\n",
    "        axes[row, col].set_xlabel('Datetime')\n",
    "        axes[row, col].set_ylabel(f'{title} ({unit})')\n",
    "        axes[row, col].grid(True)\n",
    "        axes[row, col].legend()\n",
    "        \n",
    "    #---------------------------------------------- File output\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    # Plot 2: Daily Patterns\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(15, 30))\n",
    "    fig.suptitle(f\"Daily Patterns of {data_type} Measurements - mean & std \", fontsize=16, y=1.02)\n",
    "\n",
    "    # Create hour column for grouping\n",
    "    df_hour = df.copy()\n",
    "    df_hour['hour'] = df_hour.index.hour # set index to hour (of day)\n",
    "\n",
    "    \n",
    "    for i, measurement in enumerate(measurements):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        \n",
    "        # Calculate hourly statistics\n",
    "        hourly_stats = df_hour.groupby('hour')[measurement].agg(['mean', 'std'])\n",
    "        \n",
    "        # Plot mean with standard deviation\n",
    "        axes[row, col].plot(hourly_stats.index, hourly_stats['mean'], 'b-', label='Mean')\n",
    "        axes[row, col].fill_between(\n",
    "            hourly_stats.index,\n",
    "            hourly_stats['mean'] - hourly_stats['std'],\n",
    "            hourly_stats['mean'] + hourly_stats['std'],\n",
    "            alpha=0.2,\n",
    "            label='±1 std'\n",
    "        )\n",
    "        \n",
    "        axes[row, col].set_title(f'Daily {measurement.capitalize()} Pattern')\n",
    "        axes[row, col].set_xlabel('Hour of Day')\n",
    "        axes[row, col].set_ylabel(measurement)\n",
    "        axes[row, col].grid(True)\n",
    "        axes[row, col].legend()\n",
    "\n",
    "\n",
    "    #---------------------------------------------- File output\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "\n",
    "    # Plot 3: Hour-wise Distributions\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(15, 30))\n",
    "    fig.suptitle(f\" {data_type} Measurement Distributions by Hour - Boxplots\", fontsize=16, y=1.02)\n",
    "        \n",
    "    for i,(measure, (title, unit)) in enumerate(measurements.items()):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "\n",
    "        #calculates and plots hou wise boxplot\n",
    "        df_hour.boxplot(\n",
    "            ax=axes[row, col],\n",
    "            column=measure,\n",
    "            by='hour'\n",
    "        )\n",
    "        axes[row, col].set_title(f'Daily Pattern of {title} ')\n",
    "        axes[row, col].set_xlabel('Hour of Day')\n",
    "        axes[row, col].set_ylabel(f'{title} ({unit})')\n",
    "        axes[row, col].grid(True)\n",
    "\n",
    "    #---------------------------------------------- File output\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "\n",
    "    # Plot 4 Histograms - Distribution\n",
    "    fig, axes = plt.subplots(4,2, figsize = (15, 25))\n",
    "    fig.suptitle(f\"Sensor {data_type} Measurements Distributions\", fontsize = 14)\n",
    "\n",
    "    for i,(measure, (title, unit)) in enumerate(measurements.items()):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        bin_num = 50 # was determined by trial an error\n",
    "        \n",
    "        # 200 bins are created for each measurement \n",
    "        axes[row, col].hist(df[measure], bins = bin_num*4, density = True, alpha = 0.7, label = 'Histogram')\n",
    "        axes[row, col].set_title(f'Distribution of {title} ')\n",
    "        axes[row, col].set_xlabel( f'{title} ({unit})')\n",
    "        axes[row, col].set_ylabel('Density')\n",
    "        axes[row, col].grid(True)\n",
    "        \n",
    "        #second axis for line graph\n",
    "        # 50 points for the line are used better average behaviour\n",
    "        ax_2 = axes[row, col].twinx()\n",
    "        counts, bins = np.histogram(df[measure], bins = bin_num)\n",
    "        bin_centers = (bins[:-1] + bins [1:]) / 2\n",
    "        ax_2.plot(bin_centers, counts/counts.sum(), 'r-', lw = 2, label = 'Distribution')        \n",
    "        ax_2.tick_params(axis='y', labelcolor='r')\n",
    "        ax_2.legend()\n",
    "\n",
    "    #---------------------------------------------- File output\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    # Plot 5: Correlation Analysis\n",
    "    fig, (ax) = plt.subplots(1, 1, figsize=(15, 10))\n",
    "    fig.suptitle(f\"Correlation Analysis - of {data_type} Measurements Correlations\", y=1.02, fontsize=16)\n",
    "\n",
    "    # Correlations heatmap\n",
    "    sns.heatmap(\n",
    "        df[measurements.keys()].corr(),\n",
    "        annot=True,\n",
    "        cmap='coolwarm',\n",
    "        center=0,\n",
    "        fmt='.2f',\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    #---------------------------------------------- File output\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "    # Plot 6 Hexbins    \n",
    "    measure = list(measurements.keys()) #[\"load-15m\",\"memory_used_pct\",\"cpu-user\",\"cpu-system\",\"sys-thermal\",\"sys-interrupt-rate\",\"disk-io-time\"] \n",
    "    pairs = [(measure[i], measure[j]) for i in range(len(measure)) for j in range(i + 1, len(measure))] # https://www.w3schools.com/python/python_lists_comprehension.asp\n",
    "\n",
    "    n_rows = 7\n",
    "    n_cols = 3\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    fig.suptitle(f\"Hexbins of {data_type} Measurements\", y=1.02, fontsize=16)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Loop over pairs\n",
    "    for i, (measure1, measure2) in enumerate(pairs):\n",
    "        ax = axes[i]\n",
    "        x = df[measure1]\n",
    "        y = df[measure2]\n",
    "        \n",
    "        title1, unit1 = measurements[measure1]\n",
    "        title2, unit2 = measurements[measure2]\n",
    "\n",
    "        hb = ax.hexbin(x, y, gridsize=100, cmap='viridis')\n",
    "        ax.set_xlabel(f'{title1} ({unit1})')\n",
    "        ax.set_ylabel(f'{title2} ({unit2})')\n",
    "        ax.set_title(f'Hexbin: {title1} vs {title2}')\n",
    "        \n",
    "        fig.colorbar(hb, ax=ax)\n",
    "        \n",
    "    #---------------------------------------------- File output\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "\n",
    "    # Plot 7: Scatter Matrix\n",
    "    # Get data without duplicates by taking mean for each timestamp\n",
    "    df_plot = df.groupby(df.index)[measure].mean()\n",
    "    try:\n",
    "        pp = sns.pairplot(data=df_plot,\n",
    "                            diag_kind='kde',\n",
    "                            plot_kws={'alpha': 0.5, 's': 20},\n",
    "                            height = 3,\n",
    "                            corner=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create scatter matrix plot: {str(e)}\")\n",
    "\n",
    "    fig = pp.figure\n",
    "    fig.suptitle('Scatter Matrix of Raw Measurements', y=1.02, fontsize=16)\n",
    "    \n",
    "    #---------------------------------------------- File output\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=200, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    save_image(image_names, out_dir, out_name)\n",
    "    if not show_plots:\n",
    "        plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called with a file: ./data_processed/system-19.csv\n",
      "./visu_only_processed/system-19_plot_0.png\n",
      "./visu_only_processed/system-19_plot_1.png\n",
      "./visu_only_processed/system-19_plot_2.png\n",
      "./visu_only_processed/system-19_plot_3.png\n",
      "./visu_only_processed/system-19_plot_4.png\n",
      "./visu_only_processed/system-19_plot_5.png\n",
      "./visu_only_processed/system-19_plot_6.png\n",
      "./visu_only_processed/system-19_allPlots.pdf\n"
     ]
    }
   ],
   "source": [
    "# testing \n",
    "visu_raw_data(True, pathProcessd, present,None, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Visualization of Raw\n",
    "# for file in files:\n",
    "#     visu_raw_data(False, pathFilter,file,None)\n",
    "#     plt.close(\"all\") #for safety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "- **2.1: Original data quality analysis (including visualization)**\n",
    "> - Has spikes (outliers) especially bad one in sys-interrupt and CPU-user\n",
    "> - Includes Nan- or empty-entries\n",
    "> - large hour-wise distributions due to the outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "\n",
    "- thresholds- and IQR-method\n",
    "- aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(show_process_status: bool, df:pd.DataFrame, column: str) -> tuple:\n",
    "    \"\"\"Remove outliers using IQR method. \n",
    "    Why this method: https://builtin.com/articles/1-5-iqr-rule \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    show_process_status : bool\n",
    "        print status in console\n",
    "    df : pd.DataFrame\n",
    "        input data for cleaning\n",
    "    column: str\n",
    "        current column to \"look at\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        (pd.Series, pd.Series)\n",
    "            cleaned data , outliers\n",
    "    \"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    # decision range: lower & upper bound\n",
    "    # (*1.5) -> rule comes closest to Gaussian distribution concerning the outlier detection (*1.7 = 3 sigma)\n",
    "    valid_mask = (df[column] >= Q1 - 1.5*IQR) & (df[column] <= Q3 + 1.5*IQR)\n",
    "    invalid_count = (~valid_mask).sum()\n",
    "    if show_process_status:\n",
    "        print(f\"IQR: Removing {invalid_count} outliers from {column}\")\n",
    "    return df[column].where(valid_mask, np.nan), df[column].where(~valid_mask)\n",
    "\n",
    "def handle_missing_values(data: pd.DataFrame, column: str, max_gap: int = 8) -> pd.Series: # not used\n",
    "    \"\"\"Interpolate missing values with limit.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        _description_\n",
    "    column : str\n",
    "        _description_\n",
    "    max_gap: int, optional\n",
    "        _description_ (Defaults to 8.)\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    pd.Series : \n",
    "        _description_\n",
    "    \"\"\"\n",
    "    return data[column].interpolate(\n",
    "        method='linear',\n",
    "        limit=max_gap  # Only fill gaps up to 8 points\n",
    "    )\n",
    "\n",
    "def preprocess_system_data(show_process_status: bool, file_dir: str, file_name: str, df_arg: pd.DataFrame = None) -> list:\n",
    "    \"\"\"Preprocess system performance data.\n",
    "    Cleans data with:\n",
    "          * Invalid values removed\n",
    "          * Duplicates handled\n",
    "          * Outliers removed\n",
    "          * Missing values interpolated\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    show_process_status: bool\n",
    "        ...\n",
    "    file_dir : str\n",
    "        Path to the CSV data file location (directory)\n",
    "    file_name : str\n",
    "        Name of the specified CSV file\n",
    "    \n",
    "    optional\n",
    "    df_arg: pd.DataFrame\n",
    "        output from load_system_data()\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_original\n",
    "        ...\n",
    "    df_cleaned\n",
    "        ...\n",
    "    \n",
    "    str: filename\n",
    "        ...\n",
    "    \"\"\"\n",
    "    # Check DataFrame was passed\n",
    "    if isinstance(df_arg, pd.DataFrame):\n",
    "        df = df_arg\n",
    "        print(\"Function called with a DataFrame.\")\n",
    "    else:\n",
    "        # Attempt to read the DataFrame from file\n",
    "        try:\n",
    "            file_path = file_dir + file_name\n",
    "            df = pd.read_csv(file_path, delimiter = \",\",usecols=[\"datetime\",\"load-15m\",\"memory_used_pct\",\"cpu-user\",\"cpu-system\",\"sys-thermal\",\"sys-interrupt-rate\",\"server-up\",\"disk-io-time\"])\n",
    "            print(f\"Function called with a file: {file_path}\")\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "            df.set_index('datetime', inplace=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading the file: {e}\")\n",
    "            return None\n",
    "    # Store original data\n",
    "    df_original = df.copy()\n",
    "    df_outliers = df.copy()\n",
    "    out_dir = pathProcessd\n",
    "    \n",
    "    valid_ranges = {\n",
    "        \"load-15m\":  (0, 1.0), \n",
    "        \"memory_used_pct\": (0, 100),\n",
    "        \"cpu-user\": (0.0, 2.0),\n",
    "        \"cpu-system\": (0.0, 2.0),\n",
    "        \"sys-thermal\": (-10, 10),\n",
    "        \"sys-interrupt-rate\": (0, 100000),\n",
    "        \"disk-io-time\": (0, 1.0),\n",
    "        \"server-up\": (0, 2)\n",
    "    }\n",
    "    columns = list(valid_ranges.keys())\n",
    "    \n",
    "    # 1. Handle invalid values\n",
    "    #(thresholds for sensible bounds determined by examining raw datasets)\n",
    "    for column, (min_val, max_val) in valid_ranges.items():\n",
    "        invalid_mask = (df[column] < min_val) | (df[column] > max_val)\n",
    "        if show_process_status:\n",
    "            print(f\"Ranges: Removing {invalid_mask.sum()} invalid values from {column}\")\n",
    "        df.loc[invalid_mask, column] = np.nan\n",
    "    \n",
    "    # 2. Handle duplicates\n",
    "    if show_process_status:\n",
    "        print(\"Handling duplicates ...\")\n",
    "    df = df.groupby(['datetime', 'server-up']).agg({ #Groups df by combination of: datetime and server-up\n",
    "        'load-15m': 'mean',\n",
    "        'memory_used_pct': 'mean',\n",
    "        'cpu-user': 'mean',\n",
    "        \"cpu-system\": 'mean',\n",
    "        'sys-thermal': 'mean' ,\n",
    "        \"sys-interrupt-rate\": 'mean',\n",
    "        \"disk-io-time\": 'mean'\n",
    "    }).reset_index()\n",
    "    #Result: row unique combination of datetime and server-up -> aggregated by mean (2.5 | 3.5 -> 3.0)\n",
    "        \n",
    "    # 3. Remove outliers\n",
    "    for column in columns:\n",
    "        df[column],df_outliers[column] = remove_outliers_iqr(show_process_status, df, column)\n",
    "    # testing df.to_csv(\"noHandling_data.csv\", index=False)\n",
    "    \n",
    "    # 4. Handle missing values <-- just delete rows with empty entries -> no interpolation ! \n",
    "    # there is ennough data and interpolating seemed like \n",
    "    #   misrepresentation of data characteristics and \n",
    "    #   as bias introduction\n",
    "    '''if show_process_status:\n",
    "        print(\"\\nHandling missing values...\")\n",
    "        print(f\"Missing values before handling: \\n{df.isnull().sum()}\")\n",
    "    for  column, (min_val, max_val) in valid_ranges.items():\n",
    "         df[column] = handle_missing_values(df, column,4)'''\n",
    "    \n",
    "    df_cleaned = df.dropna()\n",
    "    df_cleaned.set_index('datetime', inplace=True)\n",
    "    # testing print(\"After dropping empty entries: \\n\", df_cleaned.head())\n",
    "    \n",
    "    # Sort by datetime\n",
    "    df_cleaned.sort_index(inplace=True)\n",
    "    \n",
    "    if show_process_status:\n",
    "        print(f\"Missing values after handling: \\n{df_cleaned.isnull().sum()}\")\n",
    "        print(f\"\\nOriginal shape: {df_original.shape}\")\n",
    "        print(f\"Cleaned shape: {df_cleaned.shape}\")\n",
    "    \n",
    "    df_cleaned.to_csv(out_dir+file_name, index=True)\n",
    "\n",
    "    return [df_original, df_cleaned, file_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called with a file: ./data_filtered/system-19.csv\n",
      "Ranges: Removing 0 invalid values from load-15m\n",
      "Ranges: Removing 0 invalid values from memory_used_pct\n",
      "Ranges: Removing 0 invalid values from cpu-user\n",
      "Ranges: Removing 0 invalid values from cpu-system\n",
      "Ranges: Removing 0 invalid values from sys-thermal\n",
      "Ranges: Removing 0 invalid values from sys-interrupt-rate\n",
      "Ranges: Removing 0 invalid values from disk-io-time\n",
      "Ranges: Removing 0 invalid values from server-up\n",
      "Handling duplicate timestamps...\n",
      "IQR: Removing 2690 outliers from load-15m\n",
      "IQR: Removing 284 outliers from memory_used_pct\n",
      "IQR: Removing 12113 outliers from cpu-user\n",
      "IQR: Removing 12765 outliers from cpu-system\n",
      "IQR: Removing 26 outliers from sys-thermal\n",
      "IQR: Removing 8282 outliers from sys-interrupt-rate\n",
      "IQR: Removing 2732 outliers from disk-io-time\n",
      "IQR: Removing 6 outliers from server-up\n",
      "\n",
      "Handling missing values...\n",
      "Missing values before handling: \n",
      "datetime                  0\n",
      "server-up                 6\n",
      "load-15m               2690\n",
      "memory_used_pct         284\n",
      "cpu-user              12113\n",
      "cpu-system            12765\n",
      "sys-thermal              26\n",
      "sys-interrupt-rate     8282\n",
      "disk-io-time           2732\n",
      "dtype: int64\n",
      "Missing values after handling: \n",
      "server-up             0\n",
      "load-15m              0\n",
      "memory_used_pct       0\n",
      "cpu-user              0\n",
      "cpu-system            0\n",
      "sys-thermal           0\n",
      "sys-interrupt-rate    0\n",
      "disk-io-time          0\n",
      "dtype: int64\n",
      "\n",
      "Original shape: (86383, 8)\n",
      "Cleaned shape: (68211, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[                     load-15m  sys-interrupt-rate  sys-thermal  disk-io-time  \\\n",
       " datetime                                                                       \n",
       " 1970-01-01 00:00:00      0.02             1322.05         0.00        0.0002   \n",
       " 1970-01-01 00:00:30      0.01             1236.20         1.40        0.0002   \n",
       " 1970-01-01 00:01:00      0.01             1266.25         0.00        0.0002   \n",
       " 1970-01-01 00:01:30      0.01             1248.10         1.45        0.0002   \n",
       " 1970-01-01 00:02:00      0.01             1290.00         1.45        0.0002   \n",
       " ...                       ...                 ...          ...           ...   \n",
       " 1970-01-30 23:58:00      0.00             1267.25         1.45        0.0006   \n",
       " 1970-01-30 23:58:30      0.00             1266.45         1.50        0.0002   \n",
       " 1970-01-30 23:59:00      0.00             1354.75         1.50        0.0004   \n",
       " 1970-01-30 23:59:30      0.00             1276.20         1.55        0.0006   \n",
       " 1970-01-31 00:00:00      0.00             1289.25         1.55        0.0008   \n",
       " \n",
       "                      cpu-system  cpu-user  server-up  memory_used_pct  \n",
       " datetime                                                               \n",
       " 1970-01-01 00:00:00      0.0255    0.0320          2        15.024197  \n",
       " 1970-01-01 00:00:30      0.0210    0.0240          2        15.024452  \n",
       " 1970-01-01 00:01:00      0.0220    0.0235          2        15.025318  \n",
       " 1970-01-01 00:01:30      0.0245    0.0250          2        15.064915  \n",
       " 1970-01-01 00:02:00      0.0305    0.0335          2        15.068279  \n",
       " ...                         ...       ...        ...              ...  \n",
       " 1970-01-30 23:58:00      0.0415    0.0425          2        23.212204  \n",
       " 1970-01-30 23:58:30      0.0420    0.0435          2        23.207464  \n",
       " 1970-01-30 23:59:00      0.0395    0.0460          2        23.194826  \n",
       " 1970-01-30 23:59:30      0.0455    0.0430          2        23.291347  \n",
       " 1970-01-31 00:00:00      0.0485    0.0390          2        23.290226  \n",
       " \n",
       " [86383 rows x 8 columns],\n",
       "                      server-up  load-15m  memory_used_pct  cpu-user  \\\n",
       " datetime                                                              \n",
       " 1970-01-01 00:00:00        2.0      0.02        15.024197    0.0320   \n",
       " 1970-01-01 00:00:30        2.0      0.01        15.024452    0.0240   \n",
       " 1970-01-01 00:01:00        2.0      0.01        15.025318    0.0235   \n",
       " 1970-01-01 00:01:30        2.0      0.01        15.064915    0.0250   \n",
       " 1970-01-01 00:02:00        2.0      0.01        15.068279    0.0335   \n",
       " ...                        ...       ...              ...       ...   \n",
       " 1970-01-30 20:53:00        2.0      0.17        16.842316    0.0410   \n",
       " 1970-01-30 20:53:30        2.0      0.17        16.952750    0.0410   \n",
       " 1970-01-30 20:54:00        2.0      0.16        17.044635    0.0470   \n",
       " 1970-01-30 20:54:30        2.0      0.16        17.030467    0.0310   \n",
       " 1970-01-30 20:55:00        2.0      0.15        17.126734    0.0320   \n",
       " \n",
       "                      cpu-system  sys-thermal  sys-interrupt-rate  disk-io-time  \n",
       " datetime                                                                        \n",
       " 1970-01-01 00:00:00      0.0255         0.00             1322.05        0.0002  \n",
       " 1970-01-01 00:00:30      0.0210         1.40             1236.20        0.0002  \n",
       " 1970-01-01 00:01:00      0.0220         0.00             1266.25        0.0002  \n",
       " 1970-01-01 00:01:30      0.0245         1.45             1248.10        0.0002  \n",
       " 1970-01-01 00:02:00      0.0305         1.45             1290.00        0.0002  \n",
       " ...                         ...          ...                 ...           ...  \n",
       " 1970-01-30 20:53:00      0.0380         0.05             1291.50        0.0000  \n",
       " 1970-01-30 20:53:30      0.0395         0.05             1268.60        0.0000  \n",
       " 1970-01-30 20:54:00      0.0410         0.05             1261.70        0.0002  \n",
       " 1970-01-30 20:54:30      0.0210         0.10             1244.75        0.0000  \n",
       " 1970-01-30 20:55:00      0.0180         1.55             1169.05        0.0000  \n",
       " \n",
       " [68211 rows x 8 columns],\n",
       " 'system-19.csv']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tesing\n",
    "preprocess_system_data(True, pathFilter,present,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading the file: [Errno 2] No such file or directory: './data_filtered/s'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m      4\u001b[0m   anylist \u001b[38;5;241m=\u001b[39m preprocess_system_data(\u001b[38;5;28;01mFalse\u001b[39;00m, pathFilter,file,\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m   \u001b[38;5;28mprint\u001b[39m(anylist[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m : \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, anylist[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe(),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, anylist[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe())\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Run processing\n",
    "# anylist = []\n",
    "# for file in files:\n",
    "#   anylist = preprocess_system_data(False, pathFilter,file,None)\n",
    "#   print(anylist[0], \" : \\n\", anylist[1].describe(),\"\\n\", anylist[2].describe()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: if processed data makes sense \n",
    "#cache_list = preprocess_system_data(True, pathFilter,\"system-1.csv\",None)\n",
    "#visu_raw_data(True, None,None,pd.DataFrame(cache_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Visualization of Processed\n",
    "# for file in files:\n",
    "#     visu_raw_data(False, pathProcessd,file,None, False)\n",
    "#     plt.close(\"all\") #for safety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loadfile\n",
    "\n",
    "- in case different sets shall be compared. \n",
    "- if data is to be loaded into a dataframe instead of directly accessed by a function.\n",
    "- Otherwise visu_processed_data will be called directly after preprocess_system_data(). Since their IOs are suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_dir: str, file_name: str, create_description: bool = False) -> tuple:\n",
    "    \"\"\"Loads file from path and returns dataframe and its name (as tuple).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_dir : str\n",
    "        Path to the CSV data file location (directory)\n",
    "    file_name : str\n",
    "        Name of the specified CSV file\n",
    "    create_description: bool\n",
    "\n",
    "    \n",
    "    Additional output\n",
    "    ----------\n",
    "    pandas.description of passed dataset into the directory the file was called from.\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    tuple(pd.DataFrame, str)\n",
    "    pd.DataFrame: _description_\n",
    "        ...\n",
    "    str: file_name\n",
    "        ...\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_path = file_dir + file_name\n",
    "        df = pd.read_csv(file_path, delimiter = \",\",usecols=[\"datetime\",\"load-15m\",\"memory_used_pct\",\"cpu-user\",\"cpu-system\",\"sys-thermal\",\"sys-interrupt-rate\",\"server-up\",\"disk-io-time\"])\n",
    "        print(f\"Function called with a file: {file_path}\")\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        df.set_index('datetime', inplace=True)\n",
    "        # File name and path -> path used => use identifier\n",
    "        file_name = file_name.replace('.csv', '')\n",
    "        if create_description:\n",
    "            df.describe().to_csv(f'{file_dir}{file_name}_desciption.csv')\n",
    "        return (df, file_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the file: {e}\")\n",
    "        return (None,None)\n",
    "    \n",
    "\n",
    "#testing\n",
    "#print(load_file(pathFilter, files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data description for all processed datasets.\n",
    "# for file in files:\n",
    "#     load_file(pathProcessd,file, True)\n",
    "#     plt.close(\"all\") #for safety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Raw & Processed\n",
    "- **2.2: Time series visualizations**\n",
    "- **2.2: Distribution analysis with histograms**\n",
    "- **2.2: Correlation analysis and heatmaps**\n",
    "- **2.2: Daily pattern analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visu_processed_data(show_plots: bool, df_original: pd.DataFrame, df_cleaned: pd.DataFrame, filename: str) -> None:\n",
    "    \"\"\"visualize original and processed test system performance data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    show_plots: bool\n",
    "        Just output files or display in notebook\n",
    "    df_cleaned: pd.DataFrame\n",
    "        ...\n",
    "    df_original: pd.DataFrame\n",
    "        ...\n",
    "    filename: str\n",
    "        ... for pdf output\n",
    "\n",
    "    Additional outputs\n",
    "    saves visualized data into dir \"./visu_processed\" by calling save_image() and cleaning temp-files with delete_images()\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    out_dir = pathVisuProcessed\n",
    "    out_name = filename.replace('.csv','')\n",
    "    image_names = []\n",
    "    image_nr = 0\n",
    "   \n",
    "    measurements = {\n",
    "        \"load-15m\": ('load-15m', '%'),\n",
    "        \"memory_used_pct\": ('memory_used_pct', '%'),\n",
    "        \"cpu-user\": ('cpu-user', 'delta-s'),\n",
    "        \"cpu-system\": ('cpu-system', 'delta-s'),\n",
    "        \"sys-thermal\": ('sys-thermal', 'avg delta-°C/min'),\n",
    "        \"sys-interrupt-rate\": ('sys-interrupt-rate', 'delta-s'),\n",
    "        \"disk-io-time\": ('disk-io-time', 'delta-s')\n",
    "        #,\"server-up\": ('server-sup', '')\n",
    "    }\n",
    "    measures = list(measurements.keys())\n",
    "\n",
    "    # Plot 1: Time-Series\n",
    "    # see general system behaviour and easy to spot anomalies \n",
    "    # e.g. data-error -> blank ... or system-3: sucessive rise in memory_usage would suggest a memeory-leak\n",
    "    # system-19 shows a clear problem around 16-17.01. visible in load-15m, memory_used and best visible in sisk-io (processed data)\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(15, 30))\n",
    "    fig.suptitle('Tme-Series - Raw Data', fontsize=16, y=1.02)\n",
    "\n",
    "    for i,(measure, (title, unit)) in enumerate(measurements.items()):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        df_original[measure].iloc[::10].plot(ax=axes[row, col], color='lightblue', alpha=0.3, label='Original')\n",
    "        df_cleaned[measure].iloc[::10].plot(ax=axes[row, col], color='green', alpha=0.5, label='Cleaned')\n",
    "        axes[row, col].set_title(f'Time-Series of {measure.upper()}')\n",
    "        axes[row, col].set_xlabel('Datetime')\n",
    "        axes[row, col].set_ylabel(f'{title} ({unit})')\n",
    "        axes[row, col].grid(True)\n",
    "        axes[row, col].legend()\n",
    "        \n",
    "    #---------------------------------------------- File output\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    # Plot 2: Daily Patterns\n",
    "    # shows hourly behaviour (mean and std) overlaying original / processed shows better (thighter) std \n",
    "    # but also might suggest loss of relevant data by processing: cpu-system behaviour and sys-interrupt\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(15, 30))\n",
    "    fig.suptitle('Daily Patterns of Raw & Processed Measurements - mean & std ', fontsize=16, y=1.02)\n",
    "\n",
    "    # Create hour column for grouping\n",
    "    df_hour_orig = df_original.copy()\n",
    "    df_hour_clean = df_cleaned.copy()\n",
    "    df_hour_orig['hour'] = df_hour_orig.index.hour\n",
    "    df_hour_clean['hour'] = df_hour_clean.index.hour\n",
    "    \n",
    "    for i, measurement in enumerate(measurements):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        \n",
    "        # Calculate hourly statistics\n",
    "        hourly_stats_orig = df_hour_orig.groupby('hour')[measurement].agg(['mean', 'std'])\n",
    "        hourly_stats_clean = df_hour_clean.groupby('hour')[measurement].agg(['mean', 'std'])\n",
    "\n",
    "        # Plot mean with standard deviation\n",
    "        axes[row, col].plot(hourly_stats_clean.index, hourly_stats_clean['mean'], \n",
    "                        'g-', label='Mean Processed')\n",
    "        axes[row, col].fill_between(\n",
    "            hourly_stats_clean.index,\n",
    "            hourly_stats_clean['mean'] - hourly_stats_clean['std'],\n",
    "            hourly_stats_clean['mean'] + hourly_stats_clean['std'],\n",
    "            alpha=0.3,\n",
    "            color='lightgreen',\n",
    "            label='±1 std Processed'\n",
    "        )\n",
    "\n",
    "        axes[row, col].plot(hourly_stats_orig.index, hourly_stats_orig['mean'], \n",
    "                        'b-', label='Mean Raw')\n",
    "        axes[row, col].fill_between(\n",
    "            hourly_stats_orig.index,\n",
    "            hourly_stats_orig['mean'] - hourly_stats_orig['std'],\n",
    "            hourly_stats_orig['mean'] + hourly_stats_orig['std'],\n",
    "            alpha=0.2,\n",
    "            color='lightblue',\n",
    "            label='±1 std Raw'\n",
    "        )\n",
    "        #ax_2.tick_params(axis='y', labelcolor='b')\n",
    "        \n",
    "        axes[row, col].set_title(f'Daily {measurement.capitalize()} Pattern')\n",
    "        axes[row, col].set_xlabel('Hour of Day')\n",
    "        axes[row, col].set_ylabel(measurement)\n",
    "        axes[row, col].grid(True)\n",
    "        axes[row, col].legend()\n",
    "\n",
    "\n",
    "    #---------------------------------------------- File output\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "\n",
    "    del df_hour_clean, df_hour_orig\n",
    "\n",
    "    # Plot 4 Histograms - Distribution (only original data)\n",
    "    # how is data distibuted, are ther multiple peaks (accumulations), \n",
    "    # problematic sice every system and every metric would need their specific amount of bins and averaging-line \n",
    "    # 50/200 was deemed the best compromise => most figures suggest normal distributions (sigular and multimodal)\n",
    "    # in case of system-19: \n",
    "    #   cpus-user &-system aswell as sys-interrupt are normal distributed. \n",
    "    #   load-15m (bad bin-amount), memory_used and sys-thermal are multi-modla \n",
    "    #   disk-io looks to be multimodal, but in this systems-case there might be a problem (low-res. sensor or bad software readout) \n",
    "    #       since compared to e.g. system-4 (two peaks) the datapoints are in suspiciously periodic (consistant) distnaces (0.002 delta-s)\n",
    "    \n",
    "    fig, axes = plt.subplots(4,2, figsize = (15, 25))\n",
    "    fig.suptitle('Sensor Processed Measurements Distributions', fontsize = 14)\n",
    "\n",
    "    for i,(measure, (title, unit)) in enumerate(measurements.items()):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        bin_num = 50\n",
    "        axes[row, col].hist(df_cleaned[measure], bins = bin_num*4, density = True, alpha = 0.7)\n",
    "        axes[row, col].set_title(f'Distribution of {title} ')\n",
    "        axes[row, col].set_xlabel( f'{title} ({unit})')\n",
    "        axes[row, col].set_ylabel('Density')\n",
    "        axes[row, col].grid(True)\n",
    "        \n",
    "        #second axis for line graph\n",
    "        ax_2 = axes[row, col].twinx()\n",
    "        counts, bins = np.histogram(df_cleaned[measure], bins = bin_num)\n",
    "        bin_centers = (bins[:-1] + bins [1:]) / 2\n",
    "        ax_2.plot(bin_centers, counts/counts.sum(), 'r-', lw = 2, label = 'Distribution')        \n",
    "        ax_2.tick_params(axis='y', labelcolor='r')\n",
    "        ax_2.legend()\n",
    "\n",
    "\n",
    "\n",
    "    #---------------------------------------------- File output\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    # Plot 5: Correlation Analysis\n",
    "    # how does one metric correlate with another ? does cpu-user lead to high load-15m ?\n",
    "    # highest correlations:     \n",
    "    #   cpu-system(/-user) & sys-interrupts \n",
    "    #   cpu-system & -user (which is to be expected) \n",
    "    #   cpu-user & memory_used (also expected as a working cpu probably requests larger data outside its L1-3 caches)\n",
    "    # sys-thermals show no correlation (counter-intuitive): assumed reason being that value is absolute temp-change -> hard to correlate cooling/heating \n",
    "    # also interesting that there is \"low\" correlation between memory_used and disk-io \n",
    "    #   -> if information is requested for a process (calculation) from disk it should be moved to RAM (system-4 correl. suggests this)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    fig.suptitle('Correlation Analysis - Original vs Cleaned', y=1.02, fontsize=16)\n",
    "\n",
    "    # Original correlations\n",
    "    sns.heatmap(\n",
    "        df_original[measures].corr(),\n",
    "        annot=True,\n",
    "        cmap='Blues', #coolwarm\n",
    "        center=0,\n",
    "        fmt='.2f',\n",
    "        ax=ax1\n",
    "    )\n",
    "    ax1.set_title('Original Data Correlations')\n",
    "\n",
    "    # Cleaned correlations\n",
    "    sns.heatmap(\n",
    "        df_cleaned[measures].corr(),\n",
    "        annot=True,\n",
    "        cmap='Greens',\n",
    "        center=0,\n",
    "        fmt='.2f',\n",
    "        ax=ax2\n",
    "    )\n",
    "    ax2.set_title('Cleaned Data Correlations')\n",
    "\n",
    "    #---------------------------------------------- File output\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    # Plot 6 Hexbins\n",
    "    # Visualizing dense (aggregating) data, see trends and clustering (relationships), color gradients make it easier to digest and interopret\n",
    "    # poor results for some relations \n",
    "    pairs = [(measures[i], measures[j]) for i in range(len(measures)) for j in range(i + 1, len(measures))]\n",
    "\n",
    "    n_rows = 7\n",
    "    n_cols = 3\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    fig.suptitle('Hexbins of Processed Measurements', y=1.02, fontsize=16)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (measure1, measure2) in enumerate(pairs):\n",
    "        ax = axes[i]\n",
    "        x = df_cleaned[measure1]\n",
    "        y = df_cleaned[measure2]\n",
    "        \n",
    "        title1, unit1 = measurements[measure1]\n",
    "        title2, unit2 = measurements[measure2]\n",
    "        \n",
    "        hb = ax.hexbin(x, y, gridsize=100, cmap='viridis')\n",
    "        ax.set_xlabel(f'{title1} ({unit1})')\n",
    "        ax.set_ylabel(f'{title2} ({unit2})')\n",
    "        ax.set_title(f'Hexbin: {title1} vs {title2}')\n",
    "        fig.colorbar(hb, ax=ax)\n",
    "        \n",
    "    #---------------------------------------------- File output\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "\n",
    "    # Plot 7: Scatter Matrix\n",
    "    # similar to hexbins -> was used to see clear distiction before and after processing\n",
    "    df_plot1 = df_original\n",
    "    df_plot1['State'] = 'raw'\n",
    "    df_plot2 = df_cleaned\n",
    "    df_plot2['State'] = 'processed'\n",
    "    df_plot1 = pd.concat([df_plot1, df_plot2])\n",
    "    \n",
    "    #testing\n",
    "    # df_plot1.to_csv(out_dir+\"concatPlot1Plot2.csv\", index=True)\n",
    "    # print(df_plot1.shape, \" and \", df_plot2.shape)\n",
    "    # print(df_plot1.head(), \"\\n\", df_plot2.head())\n",
    "    \n",
    "    del df_plot2\n",
    "    pp = None\n",
    "    try:\n",
    "        pp = sns.pairplot(data=df_plot1,\n",
    "                            diag_kind='kde',\n",
    "                            vars = measures,\n",
    "                            hue='State',\n",
    "                            markers=[\"o\",\"D\"],\n",
    "                            plot_kws={'alpha': 0.5, 's': 20},\n",
    "                            height = 3,\n",
    "                            corner=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: {str(e)}\")\n",
    "\n",
    "    fig = pp.figure\n",
    "    fig.suptitle('Scatter Matrix of Raw and Processed Measurements', y=1.02, fontsize=16)\n",
    "    \n",
    "    del df_plot1\n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=200, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    save_image(image_names, out_dir, out_name)\n",
    "\n",
    "    if not show_plots:\n",
    "        plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# testing\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# load same file filtered and processed\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m ca1 \u001b[38;5;241m=\u001b[39m load_file(pathFilter,present)\n\u001b[0;32m      5\u001b[0m ca2 \u001b[38;5;241m=\u001b[39m load_file(pathProcessd,present)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#visualize it (exporting pdf)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_file' is not defined"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "# load same file filtered and processed\n",
    "\n",
    "ca1 = load_file(pathFilter,present)\n",
    "ca2 = load_file(pathProcessd,present)\n",
    "#visualize it (exporting pdf)\n",
    "visu_processed_data(True, ca1[0], ca2[0], ca2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anylist = []\n",
    "# for file in files:\n",
    "#     anylist = preprocess_system_data(False, pathFilter,file,None)\n",
    "#     visu_processed_data(False, anylist[0],anylist[1],anylist[2])\n",
    "#     #visu_processed_data(False, preprocess_system_data(False, pathFilter,file,None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "- **2.1: Preprocessed vs original data visual analysis**\n",
    "> ...\n",
    "- 2. **Time Patterns**\n",
    "   - Consider day vs night patterns\n",
    "   - Maintain testing cycles\n",
    "   - Preserve resource patterns\n",
    "\n",
    "3. **Data Consistency**\n",
    "   - Check metric relationships\n",
    "   - Validate cleaning impact\n",
    "   - Document changes\n",
    "\n",
    "   1. Test Execution Patterns:\\\n",
    "a_) System load shows clear day/night testing cycles\n",
    "\n",
    "2. System Performance:\\\n",
    "b_) High load periods (load-15m) align with increased Memory usage\\\n",
    "c_) High memory usage periods coincide with increased CPU usage rate of change\\\n",
    "d_) Temperature change rate increases during high system load periods\n",
    "\n",
    "3. Resource Utilization:\\\n",
    "e_) Memory usage consistently increases at test start and gradually decreases towards test completion\\\n",
    "f_) System load stays within reasonable limits (<0.4) during normal operation\n",
    "\n",
    "4. System Behavior:\\\n",
    "g_) Memory usage returns to idle state levels (around 5-6%) between test cycles\\\n",
    "h_) Load, memory, and CPU metrics collectively show clear patterns distinguishing between test execution and idle periods\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Probability Analysis\n",
    "- **Threshold-based probability estimation**\n",
    "- **Cross tabulation analysis**\n",
    "- **Conditional probability analysis**\n",
    "- **Summary of observations from each task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_analysis(show_plots: bool, file_dir: str, file_name: str, df_arg: pd.DataFrame = None) -> None:\n",
    "    \"\"\"Prob analysis...\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    show_process_status: bool\n",
    "        ...\n",
    "    file_dir : str\n",
    "        Path to the CSV data file location (directory)\n",
    "    file_name : str\n",
    "        Name of the specified CSV file\n",
    "    \n",
    "    optional\n",
    "    df_arg: pd.DataFrame\n",
    "        output from load_system_data()\n",
    "\n",
    "    Additional outputs\n",
    "    saves visualized data into dir \"./visu_prob\" by calling save_image() and cleaning temp-files with delete_images()\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check DataFrame was passed\n",
    "    if isinstance(df_arg, pd.DataFrame):\n",
    "        df = df_arg\n",
    "        out_dir = \"./\"\n",
    "        out_name = \"Visu_output_noident\"\n",
    "        print(\"Function called with a DataFrame.\")\n",
    "    else:\n",
    "        # Attempt to read the DataFrame from file\n",
    "        try:\n",
    "            file_path = file_dir + file_name\n",
    "            df = pd.read_csv(file_path, delimiter = \",\",usecols=[\"datetime\",\"load-15m\",\"memory_used_pct\",\"cpu-user\",\"cpu-system\",\"sys-thermal\",\"sys-interrupt-rate\",\"server-up\",\"disk-io-time\"])\n",
    "            print(f\"Function called with a file: {file_path}\")\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "            df.set_index('datetime', inplace=True)\n",
    "            # File name and path -> path used => use identifier \n",
    "            out_dir = pathProb\n",
    "            out_name = file_name.replace('.csv', '')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading the file: {e}\")\n",
    "            return None\n",
    "        \n",
    "    image_names = []\n",
    "    image_nr = 0\n",
    "\n",
    "    measurements = {\n",
    "        \"load-15m\": ('load-15m', '%'),\n",
    "        \"memory_used_pct\": ('memory_used_pct', '%'),\n",
    "        \"cpu-user\": ('cpu-user', 'delta-s'),\n",
    "        \"cpu-system\": ('cpu-system', 'delta-s'),\n",
    "        \"sys-thermal\": ('sys-thermal', 'avg delta-°C/min'),\n",
    "        \"sys-interrupt-rate\": ('sys-interrupt-rate', 'delta-s'),\n",
    "        \"disk-io-time\": ('disk-io-time', 'delta-s')\n",
    "    }\n",
    "    measures = list(measurements.keys())\n",
    "    \n",
    "    threshold_results = pd.DataFrame(columns=[\"Measurement\", \"Threshold\", \"Threshold_Value\", \"Probability_Larger\", \"Probability_Smaller\"])\n",
    "    #cross_tab_results = pd.DataFrame()\n",
    "    conditional_prob_results = pd.DataFrame(columns=[\"Condition\", \"Probability\"])\n",
    "    # Example placeholders for figures\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    # Placeholder for populating threshold-based probability estimation results\n",
    "    for measure in measures:\n",
    "        thresholds = [('mean',df[measure].mean()), ('10%(max)', df[measure].max()*0.1), ('90%(max)',df[measure].max()*0.9)]  # Example thresholds\n",
    "        for name,threshold in thresholds:\n",
    "            # Placeholder calculation (replace with actual logic)\n",
    "            probability_1 = np.mean(df[measure] > threshold)\n",
    "            probability_2 = 1 - probability_1\n",
    "            \n",
    "            threshold = np.round(threshold, decimals=4)\n",
    "            probability_1 = np.round(probability_1, decimals=4)\n",
    "            probability_2 = np.round(probability_2, decimals=4)\n",
    "            threshold_results = pd.concat([\n",
    "                threshold_results, \n",
    "                pd.DataFrame({\"Measurement\":[measure], \"Threshold\": [name], \"Threshold_Value\": [threshold], \"Probability_Larger\": [probability_1], \"Probability_Smaller\": [probability_2]})\n",
    "            ], ignore_index=True)\n",
    "\n",
    "    # Output threshold results to a figure\n",
    "    ax.axis('off')  # Turn off axis\n",
    "    ax.table(cellText=threshold_results.values, colLabels=threshold_results.columns, loc='center')\n",
    "    ax.set_title(\"Threshold-based Probability Estimation\")\n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    crosstable_data = {}\n",
    "\n",
    "    for measure in measures:\n",
    "        threshold_mean = df[measure].mean()\n",
    "        total_count = len(df[measure])\n",
    "        larger_count = (df[measure] > threshold_mean).sum()\n",
    "        not_larger_count = total_count - larger_count\n",
    "        \n",
    "        crosstable_data[measure] = {\n",
    "            \" x > Mean\": larger_count,\n",
    "            \" x < Mean\": not_larger_count,\n",
    "            \"Total\": total_count\n",
    "        }\n",
    "\n",
    "    # Convert to DataFrame for visualization\n",
    "    crosstable_df = pd.DataFrame(crosstable_data).transpose()\n",
    "    crosstable_df.index.name = \"Measurement\"\n",
    "\n",
    "    # Display the crosstable\n",
    "    ax.axis('off')  # Hide axis\n",
    "    ax.table(\n",
    "        cellText=crosstable_df.values,\n",
    "        rowLabels=crosstable_df.index,\n",
    "        colLabels=crosstable_df.columns,\n",
    "        loc='center'\n",
    "    )\n",
    "    ax.set_title(\"Crosstable Analysis Based on Mean Threshold\")\n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(10, 20))\n",
    "    # Generate pairs\n",
    "    pairs = [(measures[i], measures[j]) for i in range(len(measures)) for j in range(i + 1, len(measures))]\n",
    "\n",
    "    conditional_prob_results = pd.DataFrame(columns=[\"Condition\", \"Probability\"])\n",
    "\n",
    "    # Loop over the pairs\n",
    "    for i, (measure1, measure2) in enumerate(pairs):\n",
    "        thresholds = [\n",
    "            (f\"P({measure1} > (mean) | {measure2} > (mean))\", lambda: np.mean((df[measure1] > df[measure1].mean()) & (df[measure2] > df[measure2].mean()))),\n",
    "            (f\"P({measure1} < (mean) | {measure2} > (mean))\", lambda: np.mean((df[measure1] < df[measure1].mean()) & (df[measure2] > df[measure2].mean()))),\n",
    "            (f\"P({measure1} > 90%(max) | {measure2} > 90%(max))\", lambda: np.mean((df[measure1] > df[measure1].max() * 0.9) & (df[measure2] > df[measure2].max() * 0.9))),\n",
    "            (f\"P({measure1} < 10%(max) | {measure2} < 10%(max))\", lambda: np.mean((df[measure1] < df[measure1].max() * 0.1) & (df[measure2] < df[measure2].max() * 0.1))),\n",
    "            (f\"P({measure1} < 10%(max) | {measure2} > 90%(max))\", lambda: np.mean((df[measure1] < df[measure1].max() * 0.1) & (df[measure2] > df[measure2].max() * 0.9))),\n",
    "            (f\"P({measure2} < 10%(max) | {measure1} > 90%(max))\", lambda: np.mean((df[measure2] < df[measure2].max() * 0.1) & (df[measure1] > df[measure1].max() * 0.9))),\n",
    "            (f\"P({measure1} > 90%(max) | {measure2} < 10%(max))\", lambda: np.mean((df[measure1] > df[measure1].max() * 0.9) & (df[measure2] < df[measure2].max() * 0.1))),\n",
    "            (f\"P({measure2} > 90%(max) | {measure1} < 10%(max))\", lambda: np.mean((df[measure2] > df[measure2].max() * 0.9) & (df[measure1] < df[measure1].max() * 0.1))),\n",
    "        ]\n",
    "\n",
    "        for condition, calc in thresholds:\n",
    "            probability = calc()\n",
    "            probability = np.round(probability, decimals=4)\n",
    "            if probability >= 0.001:  # Only add rows with probability >= 0.0001\n",
    "                conditional_prob_results = pd.concat([\n",
    "                    conditional_prob_results,\n",
    "                    pd.DataFrame({\"Condition\": [condition], \"Probability\": [probability]})\n",
    "                ], ignore_index=True)\n",
    "\n",
    "    # Display or further process the results as needed\n",
    "    ax.axis('off')\n",
    "    ax.table(cellText=conditional_prob_results.values, colLabels=conditional_prob_results.columns, loc='center')\n",
    "    ax.set_title(\"Conditional Probability Analysis\")\n",
    "    \n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    save_image(image_names, out_dir, out_name)\n",
    "    \n",
    "    if not show_plots:\n",
    "        plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:  \n",
    "    prob_analysis(False, pathProcessd, file, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "- **2.3 Summary of observations from each task**\n",
    "> ...\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Statistical Theory Applications\n",
    "- **Law of Large Numbers demonstration**\n",
    "- **Central Limit Theorem application**\n",
    "- **Result interpretation**\n",
    "> ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Regression Analysis\n",
    "- **Linear/Polynomial model selection**\n",
    "- **Model fitting and validation**\n",
    "- **Result interpretation and analysis**\n",
    "> ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
