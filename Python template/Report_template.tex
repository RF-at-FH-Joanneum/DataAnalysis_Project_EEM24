\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}

\geometry{a4paper, margin=0.9in}

\title{Data Analysis Project Report}
\author{
	Team: Ohm's squad\\ 
	Frizberg Raphael\\
	Rauch Bruno\\
	Biljesko Leo
}

\date{}

\begin{document}
	
	\maketitle
	\section{Contributions}
	\begin{itemize}
		\item Frizberg Raphael:
		\begin{itemize}
			\item Data preprocessing and Basic Analysis
			\item Visualization and Exploratory Analysis
			\item Probability Analysis
		\end{itemize}
		\item Rauch Bruno:
		\begin{itemize}
			\item Statistical Theory Applications
		\end{itemize}
		\item Biljesko Leo:
		\begin{itemize}
			\item Regression Analysis
			\item Report Writing and Documentation
		\end{itemize}
		
			\item Github Repository: (https://github.com/RF-at-FH-Joanneum/DataAnalysis\_Project\_EEM24.git)
		
		
	\end{itemize}
	
	\section{Dataset Description}
	\begin{itemize}
		\item Dataset name and source: Westermo system test dataset (https://github.com/westermo/test-results-dataset.git)
		\item Time period and sampling frequency: sampled twice per minute for a month
		\item Key variables analyzed: server-up, load-15m, memory\_used\_pct, cpu-user, cpu-system, sys-thermal, sys-interrupt-rate, disk-io-time
		\item Stats: 86,383 observations, 0 missing values, 18,172 outliers removed. 
		
	\end{itemize}
	\pagebreak
	\section{Methods and Analysis}
	
	\subsection{Data Preprocessing}
	\begin{itemize}
		\item Cleaning procedures involved removing outliers, missing values, duplicate values, invalid values, taking specific range and sorting out by datetime
		\item Outlier handling: The outliers are removed by IQR method
		\item Missing value treatment: Removed rows with empty entries (no interpolation to avoid misrepresentation of data characteristics).
		\item Data transformations: load-15m multiplied by 100, memory\_used\_pct calculated, timestamp converted to datetime.
	\end{itemize}
	
	\subsection{Exploratory Data Analysis}
	\begin{itemize}
		\item Distribution analysis: .describe() was used to calculate statistics such as: mean, median, standard deviation, min, max, etc.. Values are stored in CSV file
		\item Time series patterns: Data was grouped by hour to examine trends in variables over time. Mean and standard deviation were computed for metrics and plotted as a line graph. Further variability was added that indicates fluctuations.
		\item Correlation analysis: Correlation has been plotted using .heatmap() showing the relationship between all metrics.
		\item Key visualizations: Matplotlib and seaborn have been used to produce plots. Other visualizations have been applied such as: histograms, boxplots, heatmaps, time plots, etc..
	\end{itemize}
	
	\subsection{Statistical Analysis}
	\begin{itemize}
		\item Probability analysis: To examine probability, threshold-based probability estimation is performed. Additionally, crosstable analysis and conditional probability analysis are applied.
		\item The Law of Large Numbers is demonstrated as the observed probability converges towards the measurements mean (threshold), while the absolute error decreases towards zero as the sample size increases.
		
		\item The Central Limit Theorem is demonstrated as the histograms of the sample means become closer to the theoretical normal curves as the sample size increases.
		
		\end{itemize}
	
	\subsection{Regression Analysis}
	 \begin{itemize} 
	 	\item Polynomial regression (deg. 1–6) was used to model non-linear relationships between features and the target variable. 
	 	\item Models were evaluated using cross-validation (5-fold splits), with $R^2$ and RMSE as performance metrics. 
	 	\item Training and CV scores were analyzed to assess overfitting or underfitting, identifying degree 5 as a good balance between complexity and performance. 
	 	\item StandardScaler ensured consistent feature scaling during model fitting. 
	 	\end{itemize}
	
	\pagebreak
	\section{Key Findings}
	
	\subsection{Statistical Insights}
	
	\begin{itemize}
		\item Distribution characteristics: cpu-user \& -system, as well as sys-interrupt have normal distribution. Load-15m, memory\_used\_pct, disk-io-time and sys-thermal are multi-modal. 
		\item Significant correlations: cpu-system (-user) \& sys-interrupts, cpu-user \& memory\_used\_pct
		
	\end{itemize}
	
	\subsection{Pattern Analysis}
	\begin{itemize}
		\item Temporal patterns: By displaying the mean and standard deviation, Daily Patterns shows hourly behavior. The processed data seems tighter (lower std), suggesting higher consistency, yet perhaps at the expense of important details like CPU-system behavior and system interrupts.
		
		\item Variable relationships: The strongest relationships are between cpu-user and sys-interrupt-rate, as well as cpu-system and sys-interrupt-rate, while moderate relationships are observed between cpu-user and cpu-system, and sys-thermal and sys-interrupt-rate.
		
		\item Identified anomalies: system-19 shows a clear problem around 16-17.01. visible in load-15m, memory\_used\_pct and best visible in disk-io-time (processed data)
	\end{itemize}
	
	\subsection{Advanced Analysis Results}
	\begin{itemize}
		\item For lower polynomials, the gap between training and CV scores implies underfitting. At higher degrees, the gap reduces, but still with slight overfitting. Degree 5 seems like a best solution for a model.
		\item At degree 1, $R^2$ fails to capture data's curvature, leading to low values. With higher degrees, model starts to capture more data, and $R^2$ increase. At degrees 5 and 6, it stabilizes at around 0.629. 
		
		\item Residuals are well distributed around zero, resulting in no major errors. In Q-Q plot, residuals align well with red line, with slight tail deviations (could indicated non-normality).
	\end{itemize}
	
	\section{Summary and Conclusions}
	\begin{itemize}
		\item Main insights: Key distribution features were identified by the analysis; load-15m and sys-thermal showed multi-modal behavior, whereas variables like cpu-user and cpu-system showed normal distributions. Significant correlations were found, especially between cpu-user and sys-interrupt-rate, which offered valuable information on the dynamics of system performance. A 5th-degree model, which achieved an $R^2$ of roughly 0.629 and successfully captured the underlying data patterns, was found to be the best compromise between complexity and performance using polynomial regression analysis.
		\item Limitations: It's possible that potentially useful trends were lost as a result of the choice to exclude missing data rather than interpolate. Furthermore, eliminating outliers might have eliminated important abnormalities that could have provided more in-depth information while also correcting noise. Lastly, during data aggregation, some temporal pattern variability was decreased, which would have masked smaller features that are essential to understanding system actions.
		
	\end{itemize}
	
	% Optional section if needed
	%\section{References}
	%\begin{enumerate}
	%    \item Reference 1
	%    \item Reference 2
	%\end{enumerate}
	
\end{document}
