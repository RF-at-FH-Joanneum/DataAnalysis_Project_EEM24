{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis Assignment 4\n",
    "**Group: Ohm_Squad**\n",
    "\n",
    "**Members: Rauch,Bilijesko,Frizberg**\n",
    "\n",
    "**Datasets: Westermo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Configure plotting\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': [12, 8],\n",
    "    'figure.dpi': 150,\n",
    "    'figure.autolayout': True,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'font.size': 12\n",
    "})\n",
    "\n",
    "pathRaw = \"./data_raw/\"\n",
    "pathFilter = \"./data_filtered/\"\n",
    "pathProcessd = \"./data_processed/\"\n",
    "pathVisuRaw = \"./visu_raw/\"\n",
    "pathVisuProcessed = \"./visu_processed/\"\n",
    "pathOnlyProcessed = \"./visu_only_processed/\"\n",
    "pathProb = \"./visu_prob/\"\n",
    "\n",
    "files = [f\"system-{number}.csv\" for number in range(1, 20)]\n",
    "\n",
    "# Systems 3, 5, 6, 8, 11 and 17 do not have sys-thermal readings ! 3/5/6 -> crashes 8/11/17 -> no thermal\n",
    "remove_entries = [0, 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]\n",
    "files = [item for index, item in enumerate(files) if index not in remove_entries]\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Data Preprocessing and Basic Analysis\n",
    "- **Basic statistical analysis using pandas**\n",
    "> -> see load_system_data()\n",
    "- **Original data quality analysis (including visualization)**\n",
    "> -> see Analysis Notes after visu_raw_data()\n",
    "- **Data preprocessing**\n",
    "> -> see preprocess_system_data() and \"data_processed\"\n",
    "- **Preprocessed vs original data visual analysis**\n",
    "> -> see Analysis Notes after visu_processed_data()\n",
    "\n",
    "# 2.2 Visualization and Exploratory Analysis\n",
    "- **Time series visualizations**\n",
    "- **Distribution analysis with histograms**\n",
    "- **Correlation analysis and heatmaps**\n",
    "- **Daily pattern analysis**\n",
    "> -> see visu_processed_data() and Analysis Notes after visu_processed_data()\n",
    "- **Summary of observed patterns - similar to True/False questions**\n",
    "> -> see Analysis Notes after visu_processed_data()\n",
    "\n",
    ">All figures/plots can be accessed in \"visu_raw\", \"visu_processed\" and \"visu_only_processed\".\n",
    "\n",
    "\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Filtering\n",
    "Files are fetched from directory and prefiltering for columns of interst.\n",
    "\n",
    "Processing timestamps to datetime for usage in timeseries (and usability).\n",
    "\n",
    "Done via a function to execute for every file separately and be able to pipe if necessary.\n",
    "\n",
    "Returning the dataframe could be either dropped or caught by either a container or piped into the next function.\n",
    "\n",
    "- **2.3: Basic statistical analysis using pandas**\n",
    ">  -> output into CSV (visu_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_system_data(file_dir: str, file_name: str) -> pd.DataFrame :\n",
    "    \"\"\"Load and prepare test system performance data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_dir : str\n",
    "        Path to the CSV data file location (directory)\n",
    "    file_name : str\n",
    "        Name of the specified CSV file\n",
    "    \n",
    "    Additional outputs\n",
    "    saves filtered data into dir \"./data_filtered\"\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Raw dataframe with columns:\n",
    "        - datetime (index)\n",
    "        - load-15m\n",
    "        - memory_used_pct\n",
    "        - cpu-user\n",
    "        - cpu-system\n",
    "        - sys-thermal\n",
    "        - sys-interrupt-rate\n",
    "        - server-up\n",
    "        - disk-io-time\n",
    "    \"\"\"\n",
    "    file_path = file_dir + file_name\n",
    "\n",
    "    df = pd.read_csv(file_path, delimiter = \",\",usecols=[\"timestamp\",\n",
    "                                                         \"load-15m\",\n",
    "                                                         \"sys-mem-available\",\n",
    "                                                         \"sys-mem-total\",\n",
    "                                                         \"cpu-user\",\n",
    "                                                         \"cpu-system\",\n",
    "                                                         \"sys-thermal\",\n",
    "                                                         \"sys-interrupt-rate\",\n",
    "                                                         \"server-up\",\n",
    "                                                         \"disk-io-time\"]) # Read in data with columns\n",
    "    \n",
    "\n",
    "    \n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'], unit = 's', errors = 'coerce') # Create datetime from timestamp\n",
    "    \n",
    "    df.set_index('datetime', inplace=True) # Set datetime as index\n",
    "\n",
    "    df['memory_used_pct'] = (1 - df['sys-mem-available']/df['sys-mem-total']) * 100 # Memory usage calculation\n",
    "    df.drop([\"timestamp\",\"sys-mem-available\",\"sys-mem-total\"], axis=1, inplace=True) # Drop unneccessary data\n",
    "    \n",
    "    df.to_csv(pathFilter+file_name, index=True)\n",
    "    \n",
    "    df.describe().to_csv(f'{pathVisuRaw}{file_name}_desciption.csv')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing \n",
    "# df = load_system_data(pathRaw,\"system-3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre filter all files\n",
    "# for file in files:\n",
    "#     load_system_data(pathRaw, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Raw\n",
    "- **2.1: Original data quality analysis (including visualization)**\n",
    "- **2.2: Time series visualizations**\n",
    "- **2.2: Distribution analysis with histograms**\n",
    "- **2.2: Correlation analysis and heatmaps**\n",
    "- **2.2: Daily pattern analysis**\n",
    "\n",
    "First: Helper functions for interacting with images and os to delete temporary files.\n",
    "\n",
    "Second: Main function for visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted https://stackoverflow.com/questions/6996603/how-can-i-delete-a-file-or-folder-in-python\n",
    "def delete_images(files: list) -> None:\n",
    "    \"\"\"Deletes the files specified in the list of file paths.\n",
    "    Parameters\n",
    "    ----------\n",
    "    files: list[str]\n",
    "        List of names of image files to put into .pdf file. \n",
    "    \n",
    "    Additional output\n",
    "    ----------\n",
    "        Deltes list of images.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        None\n",
    "    \"\"\"\n",
    "    for file in files:\n",
    "        try:\n",
    "            if os.path.exists(file):\n",
    "                os.remove(file)\n",
    "                #print(f\"Deleted: {file}\")\n",
    "            else:\n",
    "                print(f\"File not found: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file}: {e}\")\n",
    "            \n",
    "# adapted https://stackoverflow.com/questions/40906463/png-images-to-one-pdf-in-python \n",
    "# and https://www.geeksforgeeks.org/save-multiple-matplotlib-figures-in-single-pdf-file-using-python/ \n",
    "def save_image(image_names: list, out_dir: str, filename: str) -> None: \n",
    "    \"\"\"Gathers multiple plt.figure obejcts and outputs thm into a pdf \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image_names: list[str]\n",
    "        List of names of image files to put into .pdf file   \n",
    "    out_dir: str\n",
    "        Path to the directory of output .pdf file\n",
    "    filename: str\n",
    "        Name of output .pdf file\n",
    "        \n",
    "    Additional output\n",
    "    ----------\n",
    "    Saves a .pdf created by multiple .pngs into specified directory\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    image_list = [] #contains opened files\n",
    "    for name in image_names:\n",
    "        print(name)\n",
    "        image_list.append(Image.open(name))\n",
    "\n",
    "    image_list[0].save(f\"{out_dir}{filename}_allPlots.pdf\", save_all=True, append_images=image_list[1:])\n",
    "    for image in image_list:\n",
    "        image.close()\n",
    "    print(f\"{out_dir}{filename}_allPlots.pdf\")\n",
    "    delete_images(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visu_raw_data(show_plots: bool, file_dir: str, file_name: str, df_arg: pd.DataFrame, isRaw: bool = True):\n",
    "    \"\"\"Load and visualize filtered and processed test system performance data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    show_plots: bool\n",
    "        Just output files or display in notebook\n",
    "    file_dir : str\n",
    "        Path to the CSV data file location (directory)\n",
    "    file_name : str\n",
    "        Name of the specified CSV file\n",
    "    isRaw : bool (Default: True)\n",
    "        function can be used to visualize any raw or processed -> changes data_type (string) and out_dir (string)\n",
    "        \n",
    "    optional\n",
    "    df_arg: pd.DataFrame\n",
    "        output from load_system_data()\n",
    "\n",
    "    Additional output\n",
    "    ----------\n",
    "    saves visualized data into dir \"./visu_raw\" by calling save_image() and cleaning temp-files with delete_images()\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check DataFrame was passed\n",
    "    if isinstance(df_arg, pd.DataFrame):\n",
    "        df = df_arg\n",
    "        # File name and path -> pd used => no identifier => using \"./\" \n",
    "        out_dir = \"./\"\n",
    "        out_name = \"Visu_output_noident\"\n",
    "        print(\"Function called with a DataFrame.\")\n",
    "    else:\n",
    "        # Attempt to read the DataFrame from file\n",
    "        try:\n",
    "            file_path = file_dir + file_name\n",
    "            df = pd.read_csv(file_path, delimiter = \",\",usecols=[\"datetime\",\"load-15m\",\"memory_used_pct\",\"cpu-user\",\"cpu-system\",\"sys-thermal\",\"sys-interrupt-rate\",\"server-up\",\"disk-io-time\"])\n",
    "            print(f\"Function called with a file: {file_path}\")\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "            df.set_index('datetime', inplace=True)\n",
    "            # File name and path -> path used => use identifier \n",
    "            out_dir = pathVisuRaw\n",
    "            out_name = file_name.replace('.csv', '')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading the file: {e}\")\n",
    "            return None\n",
    "        \n",
    "    measurements = {\n",
    "        \"load-15m\": ('load-15m', '%'),\n",
    "        \"memory_used_pct\": ('memory_used_pct', '%'),\n",
    "        \"cpu-user\": ('cpu-user', 'delta-s'),\n",
    "        \"cpu-system\": ('cpu-system', 'delta-s'),\n",
    "        \"sys-thermal\": ('sys-thermal', 'avg delta-°C/min'),\n",
    "        \"sys-interrupt-rate\": ('sys-interrupt-rate', 'delta-s'),\n",
    "        \"disk-io-time\": ('disk-io-time', 'delta-s')\n",
    "        #,\"server-up\": ('server-sup', '')\n",
    "    }\n",
    "    if (isRaw):\n",
    "        data_type = \"Raw\"\n",
    "    else:\n",
    "        data_type = \"Processed\"\n",
    "        out_dir = pathOnlyProcessed\n",
    "    \n",
    "    image_names = []\n",
    "    image_nr = 0\n",
    "    \n",
    "    # Plot 1: Time-Series\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(15, 30))\n",
    "    fig.suptitle(f\"Tme-Series - {data_type} Data\", fontsize=16, y=1.02)\n",
    "   \n",
    "    for i,(measure, (title, unit)) in enumerate(measurements.items()):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "\n",
    "        df.iloc[::10].pivot(columns='server-up', values=measure).plot(ax=axes[row, col],alpha=0.7, linewidth=2,color=['red','blue'])\n",
    "\n",
    "        axes[row, col].set_title(f'Time-Series of {measure.upper()}')\n",
    "        axes[row, col].set_xlabel('Datetime')\n",
    "        axes[row, col].set_ylabel(f'{title} ({unit})')\n",
    "        axes[row, col].grid(True)\n",
    "        axes[row, col].legend()\n",
    "        \n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    # Plot 2: Daily Patterns\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(15, 30))\n",
    "    fig.suptitle(f\"Daily Patterns of {data_type} Measurements - mean & std \", fontsize=16, y=1.02)\n",
    "\n",
    "    # Create hour column for grouping\n",
    "    df_hour = df.copy()\n",
    "    df_hour['hour'] = df_hour.index.hour\n",
    "\n",
    "    \n",
    "    for i, measurement in enumerate(measurements):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        \n",
    "        # Calculate hourly statistics\n",
    "        hourly_stats = df_hour.groupby('hour')[measurement].agg(['mean', 'std'])\n",
    "        \n",
    "        # Plot mean with standard deviation\n",
    "        axes[row, col].plot(hourly_stats.index, hourly_stats['mean'], 'b-', label='Mean')\n",
    "        axes[row, col].fill_between(\n",
    "            hourly_stats.index,\n",
    "            hourly_stats['mean'] - hourly_stats['std'],\n",
    "            hourly_stats['mean'] + hourly_stats['std'],\n",
    "            alpha=0.2,\n",
    "            label='±1 std'\n",
    "        )\n",
    "        \n",
    "        axes[row, col].set_title(f'Daily {measurement.capitalize()} Pattern')\n",
    "        axes[row, col].set_xlabel('Hour of Day')\n",
    "        axes[row, col].set_ylabel(measurement)\n",
    "        axes[row, col].grid(True)\n",
    "        axes[row, col].legend()\n",
    "\n",
    "\n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "\n",
    "    # Plot 3: Hour-wise Distributions\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(15, 30))\n",
    "    fig.suptitle(f\" {data_type} Measurement Distributions by Hour - Boxplots\", fontsize=16, y=1.02)\n",
    "        \n",
    "    for i,(measure, (title, unit)) in enumerate(measurements.items()):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        \n",
    "        df_hour.boxplot(\n",
    "            ax=axes[row, col],\n",
    "            column=measure,\n",
    "            by='hour'\n",
    "        )\n",
    "        axes[row, col].set_title(f'Daily Pattern of {title} ')\n",
    "        axes[row, col].set_xlabel('Hour of Day')\n",
    "        axes[row, col].set_ylabel(f'{title} ({unit})')\n",
    "        axes[row, col].grid(True)\n",
    "\n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "\n",
    "    # Plot 4 Histograms - Distribution\n",
    "    fig, axes = plt.subplots(4,2, figsize = (15, 25))\n",
    "    fig.suptitle(f\"Sensor {data_type} Measurements Distributions\", fontsize = 14)\n",
    "\n",
    "    for i,(measure, (title, unit)) in enumerate(measurements.items()):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        bin_num = 50\n",
    "        \n",
    "        axes[row, col].hist(df[measure], bins = bin_num*4, density = True, alpha = 0.7, label = 'Histogram')\n",
    "        axes[row, col].set_title(f'Distribution of {title} ')\n",
    "        axes[row, col].set_xlabel( f'{title} ({unit})')\n",
    "        axes[row, col].set_ylabel('Density')\n",
    "        axes[row, col].grid(True)\n",
    "        \n",
    "        #second axis for line graph\n",
    "        ax_2 = axes[row, col].twinx()\n",
    "        #print(row, col, measure, bin_num)\n",
    "        counts, bins = np.histogram(df[measure], bins = bin_num)\n",
    "        bin_centers = (bins[:-1] + bins [1:]) / 2\n",
    "        ax_2.plot(bin_centers, counts/counts.sum(), 'r-', lw = 2, label = 'Distribution')        \n",
    "        ax_2.tick_params(axis='y', labelcolor='r')\n",
    "        ax_2.legend()\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    # Plot 5: Correlation Analysis\n",
    "    fig, (ax) = plt.subplots(1, 1, figsize=(15, 10))\n",
    "    fig.suptitle(f\"Correlation Analysis - of {data_type} Measurements Correlations\", y=1.02, fontsize=16)\n",
    "\n",
    "    # Original correlations\n",
    "    sns.heatmap(\n",
    "        df[measurements.keys()].corr(),\n",
    "        annot=True,\n",
    "        cmap='coolwarm',\n",
    "        center=0,\n",
    "        fmt='.2f',\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "    # Plot 6 Hexbins    \n",
    "    measure = list(measurements.keys()) #[\"load-15m\",\"memory_used_pct\",\"cpu-user\",\"cpu-system\",\"sys-thermal\",\"sys-interrupt-rate\",\"disk-io-time\"] \n",
    "    pairs = [(measure[i], measure[j]) for i in range(len(measure)) for j in range(i + 1, len(measure))] # https://www.w3schools.com/python/python_lists_comprehension.asp\n",
    "\n",
    "    n_rows = 7\n",
    "    n_cols = 3\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    fig.suptitle(f\"Hexbins of {data_type} Measurements\", y=1.02, fontsize=16)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Loop over pairs\n",
    "    for i, (measure1, measure2) in enumerate(pairs):\n",
    "        ax = axes[i]\n",
    "        x = df[measure1]\n",
    "        y = df[measure2]\n",
    "        \n",
    "        title1, unit1 = measurements[measure1]\n",
    "        title2, unit2 = measurements[measure2]\n",
    "\n",
    "        hb = ax.hexbin(x, y, gridsize=100, cmap='viridis')\n",
    "        ax.set_xlabel(f'{title1} ({unit1})')\n",
    "        ax.set_ylabel(f'{title2} ({unit2})')\n",
    "        ax.set_title(f'Hexbin: {title1} vs {title2}')\n",
    "        \n",
    "        fig.colorbar(hb, ax=ax)\n",
    "        \n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "\n",
    "    # Plot 7: Scatter Matrix\n",
    "    # Get data without duplicates by taking mean for each timestamp\n",
    "    df_plot = df.groupby(df.index)[measure].mean()\n",
    "    try:\n",
    "        pp = sns.pairplot(data=df_plot,\n",
    "                            diag_kind='kde',\n",
    "                            plot_kws={'alpha': 0.5, 's': 20},\n",
    "                            height = 3,\n",
    "                            corner=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create scatter matrix plot: {str(e)}\")\n",
    "\n",
    "    fig = pp.figure\n",
    "    fig.suptitle('Scatter Matrix of Raw Measurements', y=1.02, fontsize=16)\n",
    "    \n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=200, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    save_image(image_names, out_dir, out_name)\n",
    "    if not show_plots:\n",
    "        plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing \n",
    "#visu_raw_data(False, pathProcessd, files[0],None, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called with a file: ./data_filtered/system-19.csv\n",
      "./visu_raw/system-19_plot_0.png\n",
      "./visu_raw/system-19_plot_1.png\n",
      "./visu_raw/system-19_plot_2.png\n",
      "./visu_raw/system-19_plot_3.png\n",
      "./visu_raw/system-19_plot_4.png\n",
      "./visu_raw/system-19_plot_5.png\n",
      "./visu_raw/system-19_plot_6.png\n",
      "./visu_raw/system-19_allPlots.pdf\n"
     ]
    }
   ],
   "source": [
    "#Run Visualization of Raw\n",
    "for file in files:\n",
    "    visu_raw_data(False, pathFilter,file,None)\n",
    "    plt.close(\"all\") #for safety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "- **2.1: Original data quality analysis (including visualization)**\n",
    "> ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "\n",
    "- thresholds- and IQR-method\n",
    "- aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(show_process_status: bool, df:pd.DataFrame, column: str) -> tuple:\n",
    "    \"\"\"Remove outliers using IQR method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    show_process_status : bool\n",
    "        print status in console\n",
    "    df : pd.DataFrame\n",
    "        input data for cleaning\n",
    "    column: str\n",
    "        current column to \"look at\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        (pd.Series, pd.Series)\n",
    "            cleaned data , outliers\n",
    "    \"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    valid_mask = (df[column] >= Q1 - 1.5*IQR) & (df[column] <= Q3 + 1.5*IQR)\n",
    "    invalid_count = (~valid_mask).sum()\n",
    "    if show_process_status:\n",
    "        print(f\"IQR: Removing {invalid_count} outliers from {column}\")\n",
    "    return df[column].where(valid_mask, np.nan), df[column].where(~valid_mask)\n",
    "\n",
    "def handle_missing_values(data: pd.DataFrame, column: str,\n",
    "                         max_gap: int = 8) -> pd.Series:\n",
    "    \"\"\"Interpolate missing values with limit.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        _description_\n",
    "    column : str\n",
    "        _description_\n",
    "    max_gap: int, optional\n",
    "        _description_ (Defaults to 8.)\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    pd.Series : \n",
    "        _description_\n",
    "    \"\"\"\n",
    "    return data[column].interpolate(\n",
    "        method='linear',\n",
    "        limit=max_gap  # Only fill gaps up to 8 points\n",
    "    )\n",
    "\n",
    "def preprocess_system_data(show_process_status: bool, file_dir: str, file_name: str, df_arg: pd.DataFrame = None) -> list:\n",
    "    \"\"\"Preprocess system performance data.\n",
    "    Cleans data with:\n",
    "          * Invalid values removed\n",
    "          * Duplicates handled\n",
    "          * Outliers removed\n",
    "          * Missing values interpolated\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    show_process_status: bool\n",
    "        ...\n",
    "    file_dir : str\n",
    "        Path to the CSV data file location (directory)\n",
    "    file_name : str\n",
    "        Name of the specified CSV file\n",
    "    \n",
    "    optional\n",
    "    df_arg: pd.DataFrame\n",
    "        output from load_system_data()\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_original\n",
    "        ...\n",
    "    df_cleaned\n",
    "        ...\n",
    "    \n",
    "    str: filename\n",
    "        ...\n",
    "    \"\"\"\n",
    "    # Check DataFrame was passed\n",
    "    if isinstance(df_arg, pd.DataFrame):\n",
    "        df = df_arg\n",
    "        print(\"Function called with a DataFrame.\")\n",
    "    else:\n",
    "        # Attempt to read the DataFrame from file\n",
    "        try:\n",
    "            file_path = file_dir + file_name\n",
    "            df = pd.read_csv(file_path, delimiter = \",\",usecols=[\"datetime\",\"load-15m\",\"memory_used_pct\",\"cpu-user\",\"cpu-system\",\"sys-thermal\",\"sys-interrupt-rate\",\"server-up\",\"disk-io-time\"])\n",
    "            print(f\"Function called with a file: {file_path}\")\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "            df.set_index('datetime', inplace=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading the file: {e}\")\n",
    "            return None\n",
    "    # Store original data\n",
    "    df_original = df.copy()\n",
    "    df_outliers = df.copy()\n",
    "    out_dir = pathProcessd\n",
    "    #columns = [\"load-15m\",\"memory_used_pct\",\"cpu-user\",\"cpu-system\",\"sys-thermal\",\"sys-interrupt-rate\",\"disk-io-time\"]\n",
    "    valid_ranges = {\n",
    "        \"load-15m\":  (0, 1.0), \n",
    "        \"memory_used_pct\": (0, 100),\n",
    "        \"cpu-user\": (0.0, 2.0),\n",
    "        \"cpu-system\": (0.0, 2.0),\n",
    "        \"sys-thermal\": (-10, 10),\n",
    "        \"sys-interrupt-rate\": (0, 100000),\n",
    "        \"disk-io-time\": (0, 1.0),\n",
    "        \"server-up\": (0, 2)\n",
    "    }\n",
    "    columns = list(valid_ranges.keys())\n",
    "    \n",
    "    # 1. Handle invalid values\n",
    "    for column, (min_val, max_val) in valid_ranges.items():\n",
    "        invalid_mask = (df[column] < min_val) | (df[column] > max_val)\n",
    "        if show_process_status:\n",
    "            print(f\"Ranges: Removing {invalid_mask.sum()} invalid values from {column}\")\n",
    "        df.loc[invalid_mask, column] = np.nan\n",
    "    \n",
    "    # 2. Handle duplicates --- needs work !! ------\n",
    "    if show_process_status:\n",
    "        print(\"Handling duplicate timestamps...\")\n",
    "    df = df.groupby(['datetime', 'server-up']).agg({\n",
    "        'load-15m': 'mean',\n",
    "        'memory_used_pct': 'mean',\n",
    "        'cpu-user': 'mean',\n",
    "        \"cpu-system\": 'mean',\n",
    "        'sys-thermal': 'mean' ,\n",
    "        \"sys-interrupt-rate\": 'mean',\n",
    "        \"disk-io-time\": 'mean'\n",
    "    }).reset_index()\n",
    "        \n",
    "    # 3. Remove outliers\n",
    "    for column in columns:\n",
    "        df[column],df_outliers[column] = remove_outliers_iqr(show_process_status, df, column)\n",
    "    # testing df.to_csv(\"noHandling_data.csv\", index=False)\n",
    "    \n",
    "    # 4. Handle missing values\n",
    "    if show_process_status:\n",
    "        print(\"\\nHandling missing values...\")\n",
    "        print(f\"Missing values before handling: \\n{df.isnull().sum()}\")\n",
    "    \n",
    "    # just delete rows with empty entries ... no interpolation ! \n",
    "    # for  column, (min_val, max_val) in valid_ranges.items():\n",
    "    #     df[column] = handle_missing_values(df, column,4)\n",
    "    \n",
    "    df_cleaned = df.dropna()\n",
    "    df_cleaned.set_index('datetime', inplace=True)\n",
    "    # testing print(\"After dropping empty entries: \\n\", df_cleaned.head())\n",
    "    \n",
    "    # Sort by datetime\n",
    "    df_cleaned.sort_index(inplace=True)\n",
    "    \n",
    "    if show_process_status:\n",
    "        print(f\"Missing values after handling: \\n{df_cleaned.isnull().sum()}\")\n",
    "        print(f\"\\nOriginal shape: {df_original.shape}\")\n",
    "        print(f\"Cleaned shape: {df_cleaned.shape}\")\n",
    "    \n",
    "    df_cleaned.to_csv(out_dir+file_name, index=True)\n",
    "\n",
    "    return [df_original, df_cleaned, file_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run processing\n",
    "# anylist = []\n",
    "# for file in files:\n",
    "#   anylist = preprocess_system_data(False, pathFilter,file,None)\n",
    "#   print(anylist[0], \" : \\n\", anylist[1].describe(),\"\\n\", anylist[2].describe()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: if processed data makes sense \n",
    "#cache_list = preprocess_system_data(True, pathFilter,\"system-1.csv\",None)\n",
    "#visu_raw_data(True, None,None,pd.DataFrame(cache_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called with a file: ./data_processed/system-19.csv\n",
      "./visu_only_processed/system-19_plot_0.png\n",
      "./visu_only_processed/system-19_plot_1.png\n",
      "./visu_only_processed/system-19_plot_2.png\n",
      "./visu_only_processed/system-19_plot_3.png\n",
      "./visu_only_processed/system-19_plot_4.png\n",
      "./visu_only_processed/system-19_plot_5.png\n",
      "./visu_only_processed/system-19_plot_6.png\n",
      "./visu_only_processed/system-19_allPlots.pdf\n"
     ]
    }
   ],
   "source": [
    "#Run Visualization of Processed\n",
    "for file in files:\n",
    "    visu_raw_data(False, pathProcessd,file,None, False)\n",
    "    plt.close(\"all\") #for safety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loadfile\n",
    "\n",
    "- in case different sets shall be compared. \n",
    "- if data is to be loaded into a dataframe instead of directly accessed by a function.\n",
    "- Otherwise visu_processed_data will be called directly after preprocess_system_data(). Since their IOs are suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_dir: str, file_name: str, create_description: bool = False) -> tuple:\n",
    "    \"\"\"Loads file from path and returns dataframe and its name (as tuple).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_dir : str\n",
    "        Path to the CSV data file location (directory)\n",
    "    file_name : str\n",
    "        Name of the specified CSV file\n",
    "    create_description: bool\n",
    "\n",
    "    \n",
    "    Additional output\n",
    "    ----------\n",
    "    pandas.description of passed dataset into the directory the file was called from.\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    tuple(pd.DataFrame, str)\n",
    "    pd.DataFrame: _description_\n",
    "        ...\n",
    "    str: file_name\n",
    "        ...\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_path = file_dir + file_name\n",
    "        df = pd.read_csv(file_path, delimiter = \",\",usecols=[\"datetime\",\"load-15m\",\"memory_used_pct\",\"cpu-user\",\"cpu-system\",\"sys-thermal\",\"sys-interrupt-rate\",\"server-up\",\"disk-io-time\"])\n",
    "        print(f\"Function called with a file: {file_path}\")\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        df.set_index('datetime', inplace=True)\n",
    "        # File name and path -> path used => use identifier\n",
    "        file_name = file_name.replace('.csv', '')\n",
    "        if create_description:\n",
    "            df.describe().to_csv(f'{file_dir}{file_name}_desciption.csv')\n",
    "        return (df, file_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the file: {e}\")\n",
    "        return (None,None)\n",
    "    \n",
    "\n",
    "#testing\n",
    "#print(load_file(pathFilter, files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data description for all processed datasets.\n",
    "# for file in files:\n",
    "#     load_file(pathProcessd,file, True)\n",
    "#     plt.close(\"all\") #for safety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Raw & Processed\n",
    "- **2.2: Time series visualizations**\n",
    "- **2.2: Distribution analysis with histograms**\n",
    "- **2.2: Correlation analysis and heatmaps**\n",
    "- **2.2: Daily pattern analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visu_processed_data(show_plots: bool, df_original: pd.DataFrame, df_cleaned: pd.DataFrame, filename: str) -> None:\n",
    "    \"\"\"Load and visualize original and processed test system performance data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    show_plots: bool\n",
    "        Just output files or display in notebook\n",
    "    df_cleaned: pd.DataFrame\n",
    "        ...\n",
    "    df_original: pd.DataFrame\n",
    "        ...\n",
    "    filename: str\n",
    "        ... for pdf output\n",
    "\n",
    "    Additional outputs\n",
    "    saves visualized data into dir \"./visu_processed\" by calling save_image() and cleaning temp-files with delete_images()\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    out_dir = pathVisuProcessed\n",
    "    out_name = filename.replace('.csv','')\n",
    "    image_names = []\n",
    "    image_nr = 0\n",
    "   \n",
    "    measurements = {\n",
    "        \"load-15m\": ('load-15m', '%'),\n",
    "        \"memory_used_pct\": ('memory_used_pct', '%'),\n",
    "        \"cpu-user\": ('cpu-user', 'delta-s'),\n",
    "        \"cpu-system\": ('cpu-system', 'delta-s'),\n",
    "        \"sys-thermal\": ('sys-thermal', 'avg delta-°C/min'),\n",
    "        \"sys-interrupt-rate\": ('sys-interrupt-rate', 'delta-s'),\n",
    "        \"disk-io-time\": ('disk-io-time', 'delta-s')\n",
    "        #,\"server-up\": ('server-sup', '')\n",
    "    }\n",
    "    measures = list(measurements.keys())\n",
    "\n",
    "    # Plot 1: Time-Series\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(15, 30))\n",
    "    fig.suptitle('Tme-Series - Raw Data', fontsize=16, y=1.02)\n",
    "\n",
    "    for i,(measure, (title, unit)) in enumerate(measurements.items()):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        df_original[measure].iloc[::10].plot(ax=axes[row, col], color='lightblue', alpha=0.3, label='Original')\n",
    "        df_cleaned[measure].iloc[::10].plot(ax=axes[row, col], color='green', alpha=0.5, label='Cleaned')\n",
    "        axes[row, col].set_title(f'Time-Series of {measure.upper()}')\n",
    "        axes[row, col].set_xlabel('Datetime')\n",
    "        axes[row, col].set_ylabel(f'{title} ({unit})')\n",
    "        axes[row, col].grid(True)\n",
    "        axes[row, col].legend()\n",
    "        \n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    # Plot 2: Daily Patterns\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(15, 30))\n",
    "    fig.suptitle('Daily Patterns of Raw & Processed Measurements - mean & std ', fontsize=16, y=1.02)\n",
    "\n",
    "    # Create hour column for grouping\n",
    "    df_hour_orig = df_original.copy()\n",
    "    df_hour_clean = df_cleaned.copy()\n",
    "    df_hour_orig['hour'] = df_hour_orig.index.hour\n",
    "    df_hour_clean['hour'] = df_hour_clean.index.hour\n",
    "    \n",
    "    for i, measurement in enumerate(measurements):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        \n",
    "        # Calculate hourly statistics\n",
    "        hourly_stats_orig = df_hour_orig.groupby('hour')[measurement].agg(['mean', 'std'])\n",
    "        hourly_stats_clean = df_hour_clean.groupby('hour')[measurement].agg(['mean', 'std'])\n",
    "\n",
    "        # Plot mean with standard deviation\n",
    "        axes[row, col].plot(hourly_stats_clean.index, hourly_stats_clean['mean'], \n",
    "                        'g-', label='Mean Processed')\n",
    "        axes[row, col].fill_between(\n",
    "            hourly_stats_clean.index,\n",
    "            hourly_stats_clean['mean'] - hourly_stats_clean['std'],\n",
    "            hourly_stats_clean['mean'] + hourly_stats_clean['std'],\n",
    "            alpha=0.3,\n",
    "            color='lightgreen',\n",
    "            label='±1 std Processed'\n",
    "        )\n",
    "        #ax_2 = axes[row, col].twinx()\n",
    "\n",
    "        axes[row, col].plot(hourly_stats_orig.index, hourly_stats_orig['mean'], \n",
    "                        'b-', label='Mean Raw')\n",
    "        axes[row, col].fill_between(\n",
    "            hourly_stats_orig.index,\n",
    "            hourly_stats_orig['mean'] - hourly_stats_orig['std'],\n",
    "            hourly_stats_orig['mean'] + hourly_stats_orig['std'],\n",
    "            alpha=0.2,\n",
    "            color='lightblue',\n",
    "            label='±1 std Raw'\n",
    "        )\n",
    "        #ax_2.tick_params(axis='y', labelcolor='b')\n",
    "        \n",
    "        axes[row, col].set_title(f'Daily {measurement.capitalize()} Pattern')\n",
    "        axes[row, col].set_xlabel('Hour of Day')\n",
    "        axes[row, col].set_ylabel(measurement)\n",
    "        axes[row, col].grid(True)\n",
    "        axes[row, col].legend()\n",
    "\n",
    "\n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "\n",
    "    del df_hour_clean, df_hour_orig\n",
    "\n",
    "    # Plot 4 Histograms - Distribution\n",
    "    fig, axes = plt.subplots(4,2, figsize = (15, 25))\n",
    "    fig.suptitle('Sensor Processed Measurements Distributions', fontsize = 14)\n",
    "\n",
    "    for i,(measure, (title, unit)) in enumerate(measurements.items()):\n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "        bin_num = 50\n",
    "        axes[row, col].hist(df_cleaned[measure], bins = bin_num*4, density = True, alpha = 0.7)\n",
    "        axes[row, col].set_title(f'Distribution of {title} ')\n",
    "        axes[row, col].set_xlabel( f'{title} ({unit})')\n",
    "        axes[row, col].set_ylabel('Density')\n",
    "        axes[row, col].grid(True)\n",
    "        \n",
    "        #second axis for line graph\n",
    "        ax_2 = axes[row, col].twinx()\n",
    "        counts, bins = np.histogram(df_cleaned[measure], bins = bin_num)\n",
    "        bin_centers = (bins[:-1] + bins [1:]) / 2\n",
    "        ax_2.plot(bin_centers, counts/counts.sum(), 'r-', lw = 2, label = 'Distribution')        \n",
    "        ax_2.tick_params(axis='y', labelcolor='r')\n",
    "        ax_2.legend()\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    # Plot 5: Correlation Analysis\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    fig.suptitle('Correlation Analysis - Original vs Cleaned', y=1.02, fontsize=16)\n",
    "\n",
    "    # Original correlations\n",
    "    sns.heatmap(\n",
    "        df_original[measures].corr(),\n",
    "        annot=True,\n",
    "        cmap='Blues', #coolwarm\n",
    "        center=0,\n",
    "        fmt='.2f',\n",
    "        ax=ax1\n",
    "    )\n",
    "    ax1.set_title('Original Data Correlations')\n",
    "\n",
    "    # Cleaned correlations\n",
    "    sns.heatmap(\n",
    "        df_cleaned[measures].corr(),\n",
    "        annot=True,\n",
    "        cmap='Greens',\n",
    "        center=0,\n",
    "        fmt='.2f',\n",
    "        ax=ax2\n",
    "    )\n",
    "    ax2.set_title('Cleaned Data Correlations')\n",
    "\n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "    # Plot 6 Hexbins\n",
    "    pairs = [(measures[i], measures[j]) for i in range(len(measures)) for j in range(i + 1, len(measures))]\n",
    "\n",
    "    n_rows = 7\n",
    "    n_cols = 3\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    fig.suptitle('Hexbins of Processed Measurements', y=1.02, fontsize=16)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (measure1, measure2) in enumerate(pairs):\n",
    "        ax = axes[i]\n",
    "        x = df_cleaned[measure1]\n",
    "        y = df_cleaned[measure2]\n",
    "        \n",
    "        title1, unit1 = measurements[measure1]\n",
    "        title2, unit2 = measurements[measure2]\n",
    "        \n",
    "        hb = ax.hexbin(x, y, gridsize=100, cmap='viridis')\n",
    "        ax.set_xlabel(f'{title1} ({unit1})')\n",
    "        ax.set_ylabel(f'{title2} ({unit2})')\n",
    "        ax.set_title(f'Hexbin: {title1} vs {title2}')\n",
    "        fig.colorbar(hb, ax=ax)\n",
    "        \n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "\n",
    "    # Plot 7: Scatter Matrix\n",
    "    df_plot1 = df_original\n",
    "    df_plot1['State'] = 'raw'\n",
    "    df_plot2 = df_cleaned\n",
    "    df_plot2['State'] = 'processed'\n",
    "    df_plot1 = pd.concat([df_plot1, df_plot2])\n",
    "    \n",
    "    #testing\n",
    "    # df_plot1.to_csv(out_dir+\"concatPlot1Plot2.csv\", index=True)\n",
    "    # print(df_plot1.shape, \" and \", df_plot2.shape)\n",
    "    # print(df_plot1.head(), \"\\n\", df_plot2.head())\n",
    "    \n",
    "    del df_plot2\n",
    "    pp = None\n",
    "    try:\n",
    "        pp = sns.pairplot(data=df_plot1,\n",
    "                            diag_kind='kde',\n",
    "                            vars = measures,\n",
    "                            hue='State',\n",
    "                            markers=[\"o\",\"D\"],\n",
    "                            plot_kws={'alpha': 0.5, 's': 20},\n",
    "                            height = 3,\n",
    "                            corner=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: {str(e)}\")\n",
    "\n",
    "    fig = pp.figure\n",
    "    fig.suptitle('Scatter Matrix of Raw and Processed Measurements', y=1.02, fontsize=16)\n",
    "    \n",
    "    del df_plot1\n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=200, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    save_image(image_names, out_dir, out_name)\n",
    "\n",
    "    if not show_plots:\n",
    "        plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "# load same file filtered and processed\n",
    "\n",
    "# ca1 = load_file(pathFilter,files[2])\n",
    "# ca2 = load_file(pathProcessd,files[2])\n",
    "# #visualize it (exporting pdf)\n",
    "# visu_processed_data(False, ca1[0], ca2[0], ca2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called with a file: ./data_filtered/system-19.csv\n",
      "./visu_processed/system-19_plot_0.png\n",
      "./visu_processed/system-19_plot_1.png\n",
      "./visu_processed/system-19_plot_2.png\n",
      "./visu_processed/system-19_plot_3.png\n",
      "./visu_processed/system-19_plot_4.png\n",
      "./visu_processed/system-19_plot_5.png\n",
      "./visu_processed/system-19_allPlots.pdf\n"
     ]
    }
   ],
   "source": [
    "anylist = []\n",
    "for file in files:\n",
    "    anylist = preprocess_system_data(False, pathFilter,file,None)\n",
    "    visu_processed_data(False, anylist[0],anylist[1],anylist[2])\n",
    "    #visu_processed_data(False, preprocess_system_data(False, pathFilter,file,None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "- **2.1: Preprocessed vs original data visual analysis**\n",
    "> ...\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Probability Analysis\n",
    "- **Threshold-based probability estimation**\n",
    "- **Cross tabulation analysis**\n",
    "- **Conditional probability analysis**\n",
    "- **Summary of observations from each task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_analysis(show_plots: bool, file_dir: str, file_name: str, df_arg: pd.DataFrame = None) -> None:\n",
    "    \"\"\"Prob analysis...\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    show_process_status: bool\n",
    "        ...\n",
    "    file_dir : str\n",
    "        Path to the CSV data file location (directory)\n",
    "    file_name : str\n",
    "        Name of the specified CSV file\n",
    "    \n",
    "    optional\n",
    "    df_arg: pd.DataFrame\n",
    "        output from load_system_data()\n",
    "\n",
    "    Additional outputs\n",
    "    saves visualized data into dir \"./visu_prob\" by calling save_image() and cleaning temp-files with delete_images()\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check DataFrame was passed\n",
    "    if isinstance(df_arg, pd.DataFrame):\n",
    "        df = df_arg\n",
    "        out_dir = \"./\"\n",
    "        out_name = \"Visu_output_noident\"\n",
    "        print(\"Function called with a DataFrame.\")\n",
    "    else:\n",
    "        # Attempt to read the DataFrame from file\n",
    "        try:\n",
    "            file_path = file_dir + file_name\n",
    "            df = pd.read_csv(file_path, delimiter = \",\",usecols=[\"datetime\",\"load-15m\",\"memory_used_pct\",\"cpu-user\",\"cpu-system\",\"sys-thermal\",\"sys-interrupt-rate\",\"server-up\",\"disk-io-time\"])\n",
    "            print(f\"Function called with a file: {file_path}\")\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "            df.set_index('datetime', inplace=True)\n",
    "            # File name and path -> path used => use identifier \n",
    "            out_dir = pathProb\n",
    "            out_name = file_name.replace('.csv', '')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading the file: {e}\")\n",
    "            return None\n",
    "        \n",
    "    image_names = []\n",
    "    image_nr = 0\n",
    "\n",
    "    measurements = {\n",
    "        \"load-15m\": ('load-15m', '%'),\n",
    "        \"memory_used_pct\": ('memory_used_pct', '%'),\n",
    "        \"cpu-user\": ('cpu-user', 'delta-s'),\n",
    "        \"cpu-system\": ('cpu-system', 'delta-s'),\n",
    "        \"sys-thermal\": ('sys-thermal', 'avg delta-°C/min'),\n",
    "        \"sys-interrupt-rate\": ('sys-interrupt-rate', 'delta-s'),\n",
    "        \"disk-io-time\": ('disk-io-time', 'delta-s')\n",
    "    }\n",
    "    measures = list(measurements.keys())\n",
    "    \n",
    "    threshold_results = pd.DataFrame(columns=[\"Measurement\", \"Threshold\", \"Threshold_Value\", \"Probability_Larger\", \"Probability_Smaller\"])\n",
    "    #cross_tab_results = pd.DataFrame()\n",
    "    conditional_prob_results = pd.DataFrame(columns=[\"Condition\", \"Probability\"])\n",
    "    # Example placeholders for figures\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    # Placeholder for populating threshold-based probability estimation results\n",
    "    for measure in measures:\n",
    "        thresholds = [('mean',df[measure].mean()), ('10%(max)', df[measure].max()*0.1), ('90%(max)',df[measure].max()*0.9)]  # Example thresholds\n",
    "        for name,threshold in thresholds:\n",
    "            # Placeholder calculation (replace with actual logic)\n",
    "            probability_1 = np.mean(df[measure] > threshold)\n",
    "            probability_2 = 1 - probability_1\n",
    "            \n",
    "            threshold = np.round(threshold, decimals=4)\n",
    "            probability_1 = np.round(probability_1, decimals=4)\n",
    "            probability_2 = np.round(probability_2, decimals=4)\n",
    "            threshold_results = pd.concat([\n",
    "                threshold_results, \n",
    "                pd.DataFrame({\"Measurement\":[measure], \"Threshold\": [name], \"Threshold_Value\": [threshold], \"Probability_Larger\": [probability_1], \"Probability_Smaller\": [probability_2]})\n",
    "            ], ignore_index=True)\n",
    "\n",
    "    # Output threshold results to a figure\n",
    "    ax.axis('off')  # Turn off axis\n",
    "    ax.table(cellText=threshold_results.values, colLabels=threshold_results.columns, loc='center')\n",
    "    ax.set_title(\"Threshold-based Probability Estimation\")\n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    crosstable_data = {}\n",
    "\n",
    "    for measure in measures:\n",
    "        threshold_mean = df[measure].mean()\n",
    "        total_count = len(df[measure])\n",
    "        larger_count = (df[measure] > threshold_mean).sum()\n",
    "        not_larger_count = total_count - larger_count\n",
    "        \n",
    "        crosstable_data[measure] = {\n",
    "            \" x > Mean\": larger_count,\n",
    "            \" x < Mean\": not_larger_count,\n",
    "            \"Total\": total_count\n",
    "        }\n",
    "\n",
    "    # Convert to DataFrame for visualization\n",
    "    crosstable_df = pd.DataFrame(crosstable_data).transpose()\n",
    "    crosstable_df.index.name = \"Measurement\"\n",
    "\n",
    "    # Display the crosstable\n",
    "    ax.axis('off')  # Hide axis\n",
    "    ax.table(\n",
    "        cellText=crosstable_df.values,\n",
    "        rowLabels=crosstable_df.index,\n",
    "        colLabels=crosstable_df.columns,\n",
    "        loc='center'\n",
    "    )\n",
    "    ax.set_title(\"Crosstable Analysis Based on Mean Threshold\")\n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(10, 20))\n",
    "    # Generate pairs\n",
    "    pairs = [(measures[i], measures[j]) for i in range(len(measures)) for j in range(i + 1, len(measures))]\n",
    "\n",
    "    conditional_prob_results = pd.DataFrame(columns=[\"Condition\", \"Probability\"])\n",
    "\n",
    "    # Loop over the pairs\n",
    "    for i, (measure1, measure2) in enumerate(pairs):\n",
    "        thresholds = [\n",
    "            (f\"P({measure1} > (mean) | {measure2} > (mean))\", lambda: np.mean((df[measure1] > df[measure1].mean()) & (df[measure2] > df[measure2].mean()))),\n",
    "            (f\"P({measure1} < (mean) | {measure2} > (mean))\", lambda: np.mean((df[measure1] < df[measure1].mean()) & (df[measure2] > df[measure2].mean()))),\n",
    "            (f\"P({measure1} > 90%(max) | {measure2} > 90%(max))\", lambda: np.mean((df[measure1] > df[measure1].max() * 0.9) & (df[measure2] > df[measure2].max() * 0.9))),\n",
    "            (f\"P({measure1} < 10%(max) | {measure2} < 10%(max))\", lambda: np.mean((df[measure1] < df[measure1].max() * 0.1) & (df[measure2] < df[measure2].max() * 0.1))),\n",
    "            (f\"P({measure1} < 10%(max) | {measure2} > 90%(max))\", lambda: np.mean((df[measure1] < df[measure1].max() * 0.1) & (df[measure2] > df[measure2].max() * 0.9))),\n",
    "            (f\"P({measure2} < 10%(max) | {measure1} > 90%(max))\", lambda: np.mean((df[measure2] < df[measure2].max() * 0.1) & (df[measure1] > df[measure1].max() * 0.9))),\n",
    "            (f\"P({measure1} > 90%(max) | {measure2} < 10%(max))\", lambda: np.mean((df[measure1] > df[measure1].max() * 0.9) & (df[measure2] < df[measure2].max() * 0.1))),\n",
    "            (f\"P({measure2} > 90%(max) | {measure1} < 10%(max))\", lambda: np.mean((df[measure2] > df[measure2].max() * 0.9) & (df[measure1] < df[measure1].max() * 0.1))),\n",
    "        ]\n",
    "\n",
    "        for condition, calc in thresholds:\n",
    "            probability = calc()\n",
    "            probability = np.round(probability, decimals=4)\n",
    "            if probability >= 0.001:  # Only add rows with probability >= 0.0001\n",
    "                conditional_prob_results = pd.concat([\n",
    "                    conditional_prob_results,\n",
    "                    pd.DataFrame({\"Condition\": [condition], \"Probability\": [probability]})\n",
    "                ], ignore_index=True)\n",
    "\n",
    "    # Display or further process the results as needed\n",
    "    ax.axis('off')\n",
    "    ax.table(cellText=conditional_prob_results.values, colLabels=conditional_prob_results.columns, loc='center')\n",
    "    ax.set_title(\"Conditional Probability Analysis\")\n",
    "    \n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    save_image(image_names, out_dir, out_name)\n",
    "    \n",
    "    if not show_plots:\n",
    "        plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called with a file: ./data_processed/system-19.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bruno\\AppData\\Local\\Temp\\ipykernel_13556\\3299685583.py:75: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  threshold_results = pd.concat([\n",
      "C:\\Users\\Bruno\\AppData\\Local\\Temp\\ipykernel_13556\\3299685583.py:154: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  conditional_prob_results = pd.concat([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./visu_prob/system-19_plot_0.png\n",
      "./visu_prob/system-19_plot_1.png\n",
      "./visu_prob/system-19_plot_2.png\n",
      "./visu_prob/system-19_allPlots.pdf\n"
     ]
    }
   ],
   "source": [
    "for file in files:  \n",
    "    prob_analysis(False, pathProcessd, file, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Statistical Theory Applications\n",
    "- **Law of Large Numbers demonstration**\n",
    "- **Central Limit Theorem application**\n",
    "- **Result interpretation**\n",
    "> ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "pathStatisticslTheory = \"./visu_statistical_theory/\"\n",
    "\n",
    "def visualize_statistical_theory(show_plots: bool, df_cleaned: pd.DataFrame, filename: str) -> None :\n",
    "    \"\"\"\n",
    "    Visualize the law of large numbers and the central limit theorem using cleaned data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    show_plots: bool\n",
    "        Just output files or display in notebook\n",
    "    df_cleaned: pd.DataFrame\n",
    "        ...\n",
    "    filename: str\n",
    "        ... for pdf output\n",
    "\n",
    "    Additional outputs\n",
    "    saves visualized data into dir \"./visu_statistical_theory\" by calling save_image() and cleaning temp-files with delete_images()\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    out_dir = pathStatisticslTheory\n",
    "    out_name = filename.replace('.csv','')\n",
    "    image_names = []\n",
    "    image_nr = 0\n",
    "   \n",
    "    measurements = {\n",
    "        \"load-15m\": ('load-15m', '%'),\n",
    "        \"memory_used_pct\": ('memory_used_pct', '%'),\n",
    "        \"cpu-user\": ('cpu-user', 'delta-s'),\n",
    "        \"cpu-system\": ('cpu-system', 'delta-s'),\n",
    "        \"sys-thermal\": ('sys-thermal', 'avg delta-°C/min'),\n",
    "        \"sys-interrupt-rate\": ('sys-interrupt-rate', 'delta-s'),\n",
    "        \"disk-io-time\": ('disk-io-time', 'delta-s')\n",
    "    }\n",
    "\n",
    "    # The Law of Large Numbers\n",
    "\n",
    "    # Setup samples and threshold\n",
    "    sample_size = 10000\n",
    "\n",
    "    # Setup plots\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(15, 30))\n",
    "    fig.suptitle(f'Demonstration of the Law of Large Numbers - Processed Data - (Sample size = {sample_size})', fontsize=16, y=1.02)\n",
    "\n",
    "    # Setup data with a copy with resettint the datetime index to an index (not sure if needed actually)\n",
    "    reset_index_cleaned = df_cleaned.reset_index(drop=True)\n",
    "\n",
    "    # Create plots for each measurement of a system\n",
    "    for i,(measure, (title, unit)) in enumerate(measurements.items()):       \n",
    "        row = i // 2\n",
    "        col = i % 2\n",
    "\n",
    "        # Calculate true probability\n",
    "        threshold = np.mean(df_cleaned[measure])\n",
    "        calc_prob = np.mean(reset_index_cleaned[measure] > threshold)\n",
    "    \n",
    "        # Calculate the observed probability that the sample value is greater that the threshold\n",
    "        observed_probs = []\n",
    "        for n in range(sample_size):\n",
    "            sample = reset_index_cleaned[measure].sample(n=n, replace=True)\n",
    "            observed_prob = np.mean(sample > threshold)\n",
    "            observed_probs.append(observed_prob)\n",
    "\n",
    "        # Generate Plot\n",
    "        axes[row, col].plot(range(sample_size), observed_probs, 'b-', label=f'Obseverd Probability')\n",
    "        axes[row, col].axhline(y=calc_prob, color='r', linestyle='--', label=f'Calculated Probability ({calc_prob:.3f})')\n",
    "        axes[row, col].set_title(f'The Law of Large Numbers of {measure.upper()}')\n",
    "        axes[row, col].set_xlabel(f'Sample Size (log scale)')\n",
    "        axes[row, col].set_ylabel(f'Probability of {title} > {threshold:.5f}')\n",
    "        axes[row, col].grid(True)\n",
    "        axes[row, col].set_xscale('log')\n",
    "        axes[row, col].legend()\n",
    "\n",
    "    #----------------------------------------------\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "    fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "    image_names.append(temp_name)\n",
    "    image_nr += 1\n",
    "    #----------------------------------------------\n",
    "\n",
    "\n",
    "    # The Central Limit Theorem\n",
    "    \n",
    "    # Setup sample sizea and number of samples\n",
    "    sample_size = [1, 5, 10, 50]       # number of data points drawn in a single run -> better approximation of the true mean\n",
    "    n_samples = 1000                          # number of times the sampling process is repeated\n",
    "    \n",
    "    # Run using differen sample sizes\n",
    "    for sample_size in sample_size :\n",
    "        # Setup figures\n",
    "        fig, axes = plt.subplots(4, 2, figsize=(15, 30))\n",
    "        fig.suptitle(f'Demonstration of the Central Limit Theorem - Processed Data - (sample_size = {sample_size}; n_samples = {n_samples})' , fontsize=16, y=1.02)\n",
    "\n",
    "        # Create Histrograms\n",
    "        for i,(measure, (title, unit)) in enumerate(measurements.items()):       \n",
    "            row = i // 2\n",
    "            col = i % 2\n",
    "\n",
    "            # Calculate Sample means\n",
    "            sample_means = np.array([np.mean(np.random.choice(df_cleaned[measure], size=sample_size)) \n",
    "                                for _ in range(n_samples)])\n",
    "\n",
    "            # Histogram of sample means\n",
    "            sns.histplot(data=sample_means, kde=True, ax=axes[row, col], label = 'Observed Distribution')\n",
    "\n",
    "            # Add theoretical normal curve\n",
    "            x = np.linspace(min(sample_means), max(sample_means), 100)\n",
    "            data_mean = np.mean(df_cleaned[measure])\n",
    "            data_std = np.std(df_cleaned[measure])\n",
    "            theoretical_std = data_std / np.sqrt(sample_size)\n",
    "            theoretical = stats.norm.pdf(x, data_mean, theoretical_std)\n",
    "            axes[row, col].plot(x, theoretical * len(sample_means) * (max(sample_means) - min(sample_means)) / 50,\n",
    "                    'r--', label='Calculated Normal')\n",
    "            axes[row, col].legend()\n",
    "\n",
    "            axes[row, col].set_title(f'Sampling Distribution of {measure}')\n",
    "            axes[row, col].set_xlabel(f'Sample Mean')\n",
    "            axes[row, col].set_ylabel(f'Frequency')\n",
    "            axes[row, col].legend()\n",
    "\n",
    "        #----------------------------------------------\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        temp_name = f\"{out_dir}{out_name}_plot_{image_nr}.png\"\n",
    "        fig.savefig(temp_name, dpi=150, bbox_inches='tight')\n",
    "        image_names.append(temp_name)\n",
    "        image_nr += 1\n",
    "        #----------------------------------------------\n",
    "\n",
    "    save_image(image_names, out_dir, out_name)\n",
    "    if not show_plots:\n",
    "        plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called with a file: ./data_filtered/system-19.csv\n",
      "./visu_statistical_theory/system-19_plot_0.png\n",
      "./visu_statistical_theory/system-19_plot_1.png\n",
      "./visu_statistical_theory/system-19_plot_2.png\n",
      "./visu_statistical_theory/system-19_plot_3.png\n",
      "./visu_statistical_theory/system-19_plot_4.png\n",
      "./visu_statistical_theory/system-19_allPlots.pdf\n"
     ]
    }
   ],
   "source": [
    "# Visualize the Statistical Theory\n",
    "anylist = []\n",
    "anylist = preprocess_system_data(False, pathFilter,file,None)\n",
    "for file in files :\n",
    "    visualize_statistical_theory(False, anylist[1],anylist[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Regression Analysis\n",
    "- **Linear/Polynomial model selection**\n",
    "- **Model fitting and validation**\n",
    "- **Result interpretation and analysis**\n",
    "> ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
